<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: 能工巧匠集 | OneV's Den]]></title>
  <link href="http://onevcat.com/category/neng-gong-qiao-jiang-ji/atom.xml" rel="self"/>
  <link href="http://onevcat.com/"/>
  <updated>2013-07-23T23:16:20+09:00</updated>
  <id>http://onevcat.com/</id>
  <author>
    <name><![CDATA[onevcat]]></name>
    <email><![CDATA[onev@onevcat.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[shader-tutorial-1]]></title>
    <link href="http://onevcat.com/2013/07/shader-tutorial-1/"/>
    <updated>2013-07-23T23:14:00+09:00</updated>
    <id>http://onevcat.com/2013/07/shader-tutorial-1</id>
    <content type="html"><![CDATA[<p>猫都能学会的Unity3D Shader入门指南（一）</p>

<p><img src="http://img.onevcat.com/2013/shader-tutorial-banner.jpg" alt="Unity Shader教程" /></p>

<h2>动机</h2>

<p>自己使用Unity3D也有一段时间了，但是很多时候是流于表面，更多地是把这个引擎简单地用作脚本控制，而对更深入一些的层次几乎没有了解。虽然说Unity引擎设计的初衷就是创建简单的不需要开发者操心的谁都能用的3D引擎，但是只是肤浅的使用，可能是无法达到随心所欲的境地的，因此，这种状况必须改变！从哪里开始呢，貌似有句话叫做会写Shader的都是高手，于是，想大概看看从Shader开始能不能使所能到达的层次再深入一些吧，再于是，有了这个系列（希望我能坚持写完它，虽然应该会拖个半年左右）。</p>

<p>Unity3D的所有渲染工作都离不开着色器（Shader），如果你和我一样最近开始对Shader编程比较感兴趣的话，可能你和我有着同样的困惑：如何开始？Unity3D提供了一些Shader的手册和文档（比如<a href="http://docs.unity3d.com/Documentation/Manual/Shaders.html">这里</a>，<a href="http://docs.unity3d.com/Documentation/Components/Built-inShaderGuide.html">这里</a>和<a href="http://docs.unity3d.com/Documentation/Components/SL-Reference.html">这里</a>），但是一来内容比较分散，二来学习阶梯稍微陡峭了些。这对于像我这样之前完全没有接触过有关内容的新人来说是相当不友好的。国内外虽然也有一些Shader的介绍和心得，但是也同样存在内容分散的问题，很多教程前一章就只介绍了基本概念，接下来马上就搬出一个超复杂的例子，对于很多基本的用法并没有解释。也许对于Shader熟练使用的开发者来说是没有问题，但是我相信像我这样的入门者也并不在少数。在多方寻觅无果后，我觉得有必要写一份教程，来以一个入门者的角度介绍一些Shader开发的基本步骤。其实与其说是教程，倒不如说是一份自我总结，希望能够帮到有需要的人。</p>

<p>所以，本“教程”的对象是</p>

<ul>
<li>总的来说是新接触Shader开发的人：也许你知道什么是Shader，也会使用别人的Shader，但是仅限于知道一些基本的内建Shader名字，从来没有打开它们查看其源码。</li>
<li>想要更多了解Shader和有需求要进行Shader开发的开发者，但是之前并没有Shader开发的经验。</li>
</ul>


<p>当然，因为我本身在Shader开发方面也是一个不折不扣的大菜鸟，本文很多内容也只是在自己的理解加上一些可能不太靠谱的求证和总结。本文中的示例应该会有更好的方式来实现，因此您是高手并且恰巧路过的话，如果有好的方式来实现某些内容，恳请您不吝留下评论，我会对本文进行不断更新和维护。</p>

<!--more-->


<h2>一些基本概念</h2>

<h3>Shader和Material</h3>

<p>如果是进行3D游戏开发的话，想必您对着两个词不会陌生。Shader（着色器）实际上就是一小段程序，它负责将输入的Mesh（网格）以指定的方式和输入的贴图或者颜色等组合作用，然后输出。绘图单元可以依据这个输出来将图像绘制到屏幕上。输入的贴图或者颜色等，加上对应的Shader，以及对Shader的特定的参数设置，将这些内容（Shader及输入参数）打包存储在一起，得到的就是一个Material（材质）。之后，我们便可以将材质赋予合适的renderer（渲染器）来进行渲染（输出）了。</p>

<p>所以说Shader并没有什么特别神奇的，它只是一段规定好输入（颜色，贴图等）和输出（渲染器能够读懂的点和颜色的对应关系）的程序。而Shader开发者要做的就是根据输入，进行计算变换，产生输出而已。</p>

<p>Shader大体上可以分为两类，简单来说</p>

<ul>
<li>表面着色器（Surface Shader） - 为你做了大部分的工作，只需要简单的技巧即可实现很多不错的效果。类比卡片机，上手以后不太需要很多努力就能拍出不错的效果。</li>
<li>片段着色器（Fragment Shader） - 可以做的事情更多，但是也比较难写。使用片段着色器的主要目的是可以在比较低的层级上进行更复杂（或者针对目标设备更高效）的开发。</li>
</ul>


<p>因为是入门文章，所以之后的介绍将主要集中在表面着色器上。</p>

<h3>Shader程序的基本结构</h3>

<p>因为着色器代码可以说专用性非常强，因此人为地规定了它的基本结构。一个普通的着色器的结构应该是这样的：
<img src="http://img.onevcat.com/2013/shader-structure.png" alt="一段Shader程序的结构" /></p>

<p>首先是一些属性定义，用来指定这段代码将有哪些输入。接下来是一个或者多个的子着色器，在实际运行中，哪一个子着色器被使用是由运行的平台所决定的。子着色器是代码的主体，每一个子着色器中包含一个或者多个的Pass。在计算着色时，平台先选择最优先可以使用的着色器，然后依次运行其中的Pass，然后得到输出的结果。最后指定一个回滚，用来处理所有Subshader都不能运行的情况（比如目标设备实在太老，所有Subshader中都有其不支持的特性）。</p>

<p>需要提前说明的是，在实际进行表面着色器的开发时，我们将直接在Subshader这个层次上写代码，系统将把我们的代码编译成若干个合适的Pass。废话到此为止，下面让我们真正实际进入Shader的世界吧。</p>

<h2>Hello Shader</h2>

<p>百行文档不如一个实例，下面给出一段简单的Shader代码，然后根据代码来验证下上面说到的结构和阐述一些基本的Shader语法。因为本文是针对Unity3D来写Shader的，所以也使用Unity3D来演示吧。首先，新建一个Shader，可以在Project面板中找到，Create，选择Shader，然后将其命名为<code>Diffuse Texture</code>：</p>

<p><img src="http://img.onevcat.com/2013/shader-create-in-unity.png" alt="在Unity3D中新建一个Shader" /></p>

<p>随便用个文本编辑器打开刚才新建的Shader：</p>

<p>```glsl
Shader "Custom/Diffuse Texture" {</p>

<pre><code>Properties {
    _MainTex ("Base (RGB)", 2D) = "white" {}
}
SubShader {
    Tags { "RenderType"="Opaque" }
    LOD 200

    CGPROGRAM
    #pragma surface surf Lambert

    sampler2D _MainTex;

    struct Input {
        float2 uv_MainTex;
    };

    void surf (Input IN, inout SurfaceOutput o) {
        half4 c = tex2D (_MainTex, IN.uv_MainTex);
        o.Albedo = c.rgb;
        o.Alpha = c.a;
    }
    ENDCG
} 
FallBack "Diffuse"
</code></pre>

<p>}</p>

<p>```</p>

<p>如果您之前没怎么看过Shader代码的话，估计细节上会看不太懂。但是有了上面基本结构的介绍，您应该可以识别出这个Shader的构成，比如一个Properties部分，一个SubShader，以及一个FallBack。另外，第一行只是这个Shader的声明并为其指定了一个名字，比如我们的实例Shader，你可以在材质面板选择Shader时在对应的位置找到这个Shader。</p>

<p><img src="http://img.onevcat.com/2013/shader-select.png" alt="在Unity3D中找到刚才新建的Shader" /></p>

<p><strong>接下来我们讲逐句讲解这个Shader，以期明了每一个语句的意义。</strong></p>

<h3>属性</h3>

<p>在<code>Properties{}</code>中定义着色器属性，在这里定义的属性将被作为输入提供给所有的子着色器。每一条属性的定义的语法是这样的：</p>

<p><code>_Name("Display Name", type) = defaultValue[{options}]</code></p>

<ul>
<li>_Name - 属性的名字，简单说就是变量名，在之后整个Shader代码中将使用这个名字来获取该属性的内容</li>
<li>Display Name - 这个字符串将显示在Unity的材质编辑器中作为Shader的使用者可读的内容</li>
<li>type - 这个属性的类型，可能的type所表示的内容有以下几种：

<ul>
<li>Color - 一种颜色，由RGBA（红绿蓝和透明度）四个量来定义；</li>
<li>2D - 一张2的阶数大小（256，512之类）的贴图。这张贴图将在采样后被转为对应基于模型UV的每个像素的颜色，最终被显示出来；</li>
<li>Rect - 一个非2阶数大小的贴图；</li>
<li>Cube - 即Cube map texture（立方体纹理），简单说就是6张有联系的2D贴图的组合，主要用来做反射效果（比如天空盒和动态反射），也会被转换为对应点的采样；</li>
<li>Range(min, max) - 一个介于最小值和最大值之间的浮点数，一般用来当作调整Shader某些特性的参数（比如透明度渲染的截止值可以是从0至1的值等）；</li>
<li>Float - 任意一个浮点数；</li>
<li>Vector - 一个四维数；</li>
</ul>
</li>
<li>defaultValue 定义了这个属性的默认值，通过输入一个符合格式的默认值来指定对应属性的初始值（某些效果可能需要某些特定的参数值来达到需要的效果，虽然这些值可以在之后在进行调整，但是如果默认就指定为想要的值的话就省去了一个个调整的时间，方便很多）。

<ul>
<li>Color - 以0～1定义的rgba颜色，比如(1,1,1,1)；</li>
<li>2D/Rect/Cube - 对于贴图来说，默认值可以为一个代表默认tint颜色的字符串，可以是空字符串或者"white","black","gray","bump"中的一个</li>
<li>Float，Range - 某个指定的浮点数</li>
<li>Vector - 一个4维数，写为 (x,y,z,w)</li>
</ul>
</li>
<li>另外还有一个{option}，它只对2D，Rect或者Cube贴图有关，在写输入时我们最少要在贴图之后写一对什么都不含的空白的{}，当我们需要打开特定选项时可以把其写在这对花括号内。如果需要同时打开多个选项，可以使用空白分隔。可能的选择有ObjectLinear, EyeLinear, SphereMap, CubeReflect, CubeNormal中的一个，这些都是OpenGL中TexGen的模式，具体的留到后面有机会再说。</li>
</ul>


<p>所以，一组属性的申明看起来也许会是这个样子的</p>

<p><code>glsl
//Define a color with a default value of semi-transparent blue
_MainColor ("Main Color", Color) = (0,0,1,0.5)
//Define a texture with a default of white
_Texture ("Texture", 2D) = "white" {}
</code></p>

<p>现在看懂上面那段Shader（以及其他所有Shader）的Properties部分应该不会有任何问题了。接下来就是SubShader部分了。</p>

<h3>Tags</h3>

<p>表面着色器可以被若干的标签（tags）所修饰，而硬件将通过判定这些标签来决定什么时候调用该着色器。比如我们的例子中SubShader的第一句</p>

<p><code>Tags { "RenderType"="Opaque" }</code></p>

<p>告诉了系统应该在渲染非透明物体时调用我们。Unity定义了一些列这样的渲染过程，与RenderType是Opaque相对应的显而易见的是<code>"RenderType" = "Transparent"</code>，表示渲染含有透明效果的物体时调用。在这里Tags其实暗示了你的Shader输出的是什么，如果输出中都是非透明物体，那写在Opaque里；如果想渲染透明或者半透明的像素，那应该写在Transparent中。</p>

<p>另外比较有用的标签还有<code>"IgnoreProjector"="True"</code>（不被<a href="http://docs.unity3d.com/Documentation/Components/class-Projector.html">Projectors</a>影响），<code>"ForceNoShadowCasting"="True"</code>（从不产生阴影）以及<code>"Queue"="xxx"</code>（指定渲染顺序队列）。这里想要着重说一下的是Queue这个标签，如果你使用Unity做过一些透明和不透明物体的混合的话，很可能已经遇到过不透明物体无法呈现在透明物体之后的情况。这种情况很可能是由于Shader的渲染顺序不正确导致的。Queue指定了物体的渲染顺序，预定义的Queue有：</p>

<ul>
<li>Background - 最早被调用的渲染，用来渲染天空盒或者背景</li>
<li>Geometry - 这是默认值，用来渲染非透明物体（普通情况下，场景中的绝大多数物体应该是非透明的）</li>
<li>AlphaTest - 用来渲染经过<a href="http://docs.unity3d.com/Documentation/Components/SL-AlphaTest.html">Alpha Test</a>的像素，单独为AlphaTest设定一个Queue是出于对效率的考虑</li>
<li>Transparent - 以从后往前的顺序渲染透明物体</li>
<li>Overlay - 用来渲染叠加的效果，是渲染的最后阶段（比如镜头光晕等特效）</li>
</ul>


<p>这些预定义的值本质上是一组定义整数，Background = 1000， Geometry = 2000, AlphaTest = 2450， Transparent = 3000，最后Overlay = 4000。在我们实际设置Queue值时，不仅能使用上面的几个预定义值，我们也可以指定自己的Queue值，写成类似这样：<code>"Queue"="Transparent+100"</code>，表示一个在Transparent之后100的Queue上进行调用。通过调整Queue值，我们可以确保某些物体一定在另一些物体之前或者之后渲染，这个技巧有时候很有用处。</p>

<h3>LOD</h3>

<p>LOD很简单，它是Level of Detail的缩写，在这里例子里我们指定了其为200（其实这是Unity的内建Diffuse着色器的设定值）。这个数值决定了我们能用什么样的Shader。在Unity的Quality Settings中我们可以设定允许的最大LOD，当设定的LOD小于SubShader所指定的LOD时，这个SubShader将不可用。Unity内建Shader定义了一组LOD的数值，我们在实现自己的Shader的时候可以将其作为参考来设定自己的LOD数值，这样在之后调整根据设备图形性能来调整画质时可以进行比较精确的控制。</p>

<ul>
<li>VertexLit及其系列 = 100</li>
<li>Decal, Reflective VertexLit = 150</li>
<li>Diffuse = 200</li>
<li>Diffuse Detail, Reflective Bumped Unlit, Reflective Bumped VertexLit = 250</li>
<li>Bumped, Specular = 300</li>
<li>Bumped Specular = 400</li>
<li>Parallax = 500</li>
<li>Parallax Specular = 600</li>
</ul>


<h3>Shader本体</h3>

<p>前面杂项说完了，终于可以开始看看最主要的部分了，也就是将输入转变为输出的代码部分。为了方便看，请容许我把上面的SubShader的主题部分抄写一遍</p>

<p>```glsl
CGPROGRAM</p>

<h1>pragma surface surf Lambert</h1>

<p>sampler2D _MainTex;</p>

<p>struct Input {</p>

<pre><code>float2 uv_MainTex;
</code></pre>

<p>};</p>

<p>void surf (Input IN, inout SurfaceOutput o) {</p>

<pre><code>half4 c = tex2D (_MainTex, IN.uv_MainTex);
o.Albedo = c.rgb;
o.Alpha = c.a;
</code></pre>

<p>}
ENDCG
```</p>

<p>还是逐行来看，首先是CGPROGRAM。这是一个开始标记，表明从这里开始是一段CG程序（我们在写Unity的Shader时用的是Cg/HLSL语言）。最后一行的ENDCG与CGPROGRAM是对应的，表明CG程序到此结束。</p>

<p>接下来是是一个编译指令：<code>#pragma surface surf Lambert</code>，它声明了我们要写一个表面Shader，并指定了光照模型。它的写法是这样的</p>

<p><code>#pragma surface surfaceFunction lightModel [optionalparams]</code></p>

<ul>
<li>surface - 声明的是一个表面着色器</li>
<li>surfaceFunction - 着色器代码的方法的名字</li>
<li>lightModel - 使用的光照模型。</li>
</ul>


<p>所以在我们的例子中，我们声明了一个表面着色器，实际的代码在surf函数中（在下面能找到该函数），使用Lambert（也就是普通的diffuse）作为光照模型。</p>

<p>接下来一句<code>sampler2D _MainTex;</code>，sampler2D是个啥？其实在CG中，sampler2D就是和texture所绑定的一个数据容器接口。等等..这个说法还是太复杂了，简单理解的话，所谓加载以后的texture（贴图）说白了不过是一块内存存储的，使用了RGB（也许还有A）通道，且每个通道8bits的数据。而具体地想知道像素与坐标的对应关系，以及获取这些数据，我们总不能一次一次去自己计算内存地址或者偏移，因此可以通过sampler2D来对贴图进行操作。更简单地理解，sampler2D就是GLSL中的2D贴图的类型，相应的，还有sampler1D，sampler3D，samplerCube等等格式。</p>

<p>解释通了sampler2D是什么之后，还需要解释下为什么在这里需要一句对<code>_MainTex</code>的声明，之前我们不是已经在<code>Properties</code>里声明过它是贴图了么。答案是我们用来实例的这个shader其实是由两个相对独立的块组成的，外层的属性声明，回滚等等是Unity可以直接使用和编译的ShaderLab；而现在我们是在<code>CGPROGRAM...ENDCG</code>这样一个代码块中，这是一段CG程序。对于这段CG程序，要想访问在<code>Properties</code>中所定义的变量的话，<strong>必须使用和之前变量相同的名字进行声明</strong>。于是其实<code>sampler2D _MainTex;</code>做的事情就是再次声明并链接了_MainTex，使得接下来的CG程序能够使用这个变量。</p>

<p>终于可以继续了。接下来是一个struct结构体。相信大家对于结构体已经很熟悉了，我们先跳过之，直接看下面的的surf函数。上面的#pragma段已经指出了我们的着色器代码的方法的名字叫做surf，那没跑儿了，就是这段代码是我们的着色器的工作核心。我们已经说过不止一次，着色器就是给定了输入，然后给出输出进行着色的代码。CG规定了声明为表面着色器的方法（就是我们这里的surf）的参数类型和名字，因此我们没有权利决定surf的输入输出参数的类型，只能按照规定写。这个规定就是第一个参数是一个Input结构，第二个参数是一个inout的SurfaceOutput结构。</p>

<p>它们分别是什么呢？Input其实是需要我们去定义的结构，这给我们提供了一个机会，可以把所需要参与计算的数据都放到这个Input结构中，传入surf函数使用；SurfaceOutput是已经定义好了里面类型输出结构，但是一开始的时候内容暂时是空白的，我们需要向里面填写输出，这样就可以完成着色了。先仔细看看INPUT吧，现在可以跳回来看上面定义的INPUT结构体了：</p>

<p>```glsl
struct Input {</p>

<pre><code>float2 uv_MainTex;
</code></pre>

<p>};
```</p>

<p>作为输入的结构体必须命名为Input，这个结构体中定义了一个float2的变量…你没看错我也没打错，就是float2，表示浮点数的float后面紧跟一个数字2，这又是什么意思呢？其实没什么魔法，float和vec都可以在之后加入一个2到4的数字，来表示被打包在一起的2到4个同类型数。比如下面的这些定义：</p>

<p><code>
//Define a 2d vector variable
vec2 coordinate;
//Define a color variable
float4 color;
//Multiply out a color
float3 multipliedColor = color.rgb * coordinate.x;
</code></p>

<p>在访问这些值时，我们即可以只使用名称来获得整组值，也可以使用下标的方式（比如.xyzw，.rgba或它们的部分比如.x等等）来获得某个值。在这个例子里，我们声明了一个叫做<code>uv_MainTex</code>的包含两个浮点数的变量。</p>

<p>如果你对3D开发稍有耳闻的话，一定不会对uv这两个字母感到陌生。UV mapping的作用是将一个2D贴图上的点按照一定规则映射到3D模型上，是3D渲染中最常见的一种顶点处理手段。在CG程序中，我们有这样的约定，在一个贴图变量（在我们例子中是<code>_MainTex</code>）之前加上uv两个字母，就代表提取它的uv值（其实就是两个代表贴图上点的二维坐标 ）。我们之后就可以在surf程序中直接通过访问uv_MainTex来取得这张贴图当前需要计算的点的坐标值了。</p>

<p>如果你坚持看到这里了，那要恭喜你，因为离最后成功读完一个Shader只有一步之遥。我们回到surf函数，它的两有参数，第一个是Input，我们已经明白了：在计算输出时Shader会多次调用surf函数，每次给入一个贴图上的点坐标，来计算输出。第二个参数是一个可写的SurfaceOutput，SurfaceOutput是预定义的输出结构，我们的surf函数的目标就是根据输入把这个输出结构填上。SurfaceOutput结构体的定义如下</p>

<p>```glsl
struct SurfaceOutput {</p>

<pre><code>half3 Albedo;     //像素的颜色
half3 Normal;     //像素的法向值
half3 Emission;   //像素的发散颜色
half Specular;    //像素的镜面高光
half Gloss;       //像素的发光强度
half Alpha;       //像素的透明度
</code></pre>

<p>};
```</p>

<p>这里的half和我们常见float与double类似，都表示浮点数，只不过精度不一样。也许你很熟悉单精度浮点数（float或者single）和双精度浮点数（double），这里的half指的是半精度浮点数，精度最低，运算性能相对比高精度浮点数高一些，因此被大量使用。</p>

<p>在例子中，我们做的事情非常简单：<code>o.Albedo = tex2D (_MainTex, IN.uv_MainTex).rgb;</code>，这里用到了一个<code>tex2d</code>函数，这是CG程序中用来在一张贴图中对一个点进行采样的方法，返回一个float4。这里对_MainTex在输入点上进行了采样，并将其颜色的rbg值赋予了输出的像素颜色。于是，着色器就明白了应当怎样工作：即找到贴图上对应的uv点，直接使用其不含透明度的颜色来进行着色，over。</p>

<h2>接下来...</h2>

<p>我想现在你已经能读懂一些最简单的Shader了，接下来我推荐的是参考Unity的<a href="http://docs.unity3d.com/Documentation/Components/SL-SurfaceShaderExamples.html">Surface Shader Examples</a>多接触一些各种各样的基本Shader。在这篇教程的基础上，配合一些google的工作，完全看懂这个shader示例页面应该不成问题。如果能做到无压力看懂，那说明你已经有良好的基础可以前进到Shader的更深的层次了（也许等不到我的下一篇教程就可以自己开始动手写些效果了）；如果暂时还是有困难，那也没有关系，Shader学习绝对是一个渐进的过程，因为有很多约定和常用技巧，多积累和实践自然会进步并掌握。</p>

<p>在接下来的教程里，打算通过介绍一些实际例子以及从基础开始实际逐步动手实现一个复杂一点的例子，让我们能看到shader在真正使用中的威力。我希望能尽快写完这个系列，但是无奈时间确实有限，所以我也不知道什么时候能出炉...写好的时候我会更改这段内容并指向新的文章。您要是担心错过的话，也可以使用<a href="http://eepurl.com/wNSkj">邮件订阅</a>或者<a href="http://onevcat.com/atom.xml">订阅本站的rss</a>(虽然Google Reader已经关了- -)。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[开发者所需要知道的iOS7 SDK新特性]]></title>
    <link href="http://onevcat.com/2013/06/developer-should-know-about-ios7/"/>
    <updated>2013-06-11T07:43:00+09:00</updated>
    <id>http://onevcat.com/2013/06/developer-should-know-about-ios7</id>
    <content type="html"><![CDATA[<p><img src="http://img.onevcat.com/2013/ios-7-logo.png" alt="iOS 7" /></p>

<p>春风又绿加州岸，物是人非又一年。WWDC 2013 keynote落下帷幕，新的iOS开发旅程也由此开启。在iOS7界面重大变革的背后，开发者们需要知道的又有哪些呢。同去年一样，我会先简单纵览地介绍iOS7中我个人认为开发者需要着重关注和学习的内容，之后再陆续对自己感兴趣章节进行探索。计划继承类似<a href="http://onevcat.com/2012/06/%E5%BC%80%E5%8F%91%E8%80%85%E6%89%80%E9%9C%80%E8%A6%81%E7%9F%A5%E9%81%93%E7%9A%84ios6-sdk%E6%96%B0%E7%89%B9%E6%80%A7/">WWDC2012的笔记</a>的形式，希望对国内开发者有所帮助。</p>

<p>相关笔记整理如下：（为了避免不必要的麻烦，所有笔记暂时就不在blog上公开全文发布了，先作为自己的笔记进行记录备份。只给出链接，之后NDA解除后再一并公开发吧..不过本站从源代码开始都是架在github上的..所以..嗯不多说了..）</p>

<ul>
<li>总览 <a href="http://onevcat.com/2013/06/developer-should-know-about-ios7/">开发者所需要知道的iOS7 SDK新特性</a></li>
<li>工具 <a href="http://onevcat.com/">WWDC2013笔记 Xcode5和ObjC新特性</a> http://onevcat.com/2013/06/new-in-xcode5-and-objc/</li>
<li>UIKit动力学 <a href="http://onevcat.com/">WWDC2013笔记 UIKit力学模型入门</a> http://onevcat.com/2013/06/uikit-dynamics-started/</li>
<li>SpriteKit入门 <a href="http://onevcat.com/">WWDC2013笔记 SpriteKit快速入门和新时代iOS游戏开发指南</a> http://onevcat.com/2013/06/sprite-kit-start/</li>
</ul>


<!--
---

### UI相关
#### 全新UI设计
iOS7最大的变化莫过于UI设计，也许你会说UI设计“这是设计师大大们应该关注的事情，不关开发者的事，我们只需要替换图片就行了”。那你就错了。UI的变化必然带来使用习惯和方式的转变，如何运用iOS7的UI，如何是自己的应用更切合新的系统，都是需要考虑的事情。另外值得注意的是，使用iOS7 SDK（现在只有Xcode5预览版提供）打包的应用在iOS7上运行时将会自动使用iOS7的新界面，所以原有应用可能需要对新界面进行重大调整。具体的iOS7中所使用的UI元素的人际交互界面文档，可以从[这里](https://developer.apple.com/library/prerelease/ios/design/index.html#//apple_ref/doc/uid/TP40013289)找到（应该是需要开发者账号才能看）。

简单总结来说，以现在上手体验看来新的UI变化改进有如下几点：

* 状态栏，导航栏和应用实际展示内容不再界限：系统自带的应用都不再区分状态栏和navigation bar，而是用统一的颜色力求简洁。这也算是一种趋势。
* BarItem的按钮全部文字化：这点做的相当坚决，所有的导航和工具条按钮都取消了拟物化，原来的文字（比如“Edit”，“Done”之类）改为了简单的文字，原来的图标（比如新建或者删除）也做了简化。
* 程序打开加入了动画：从主界面到图标所在位置的一个放大，同时显示应用的载入界面。

自己实验了几个现有的AppStore应用在iOS7上的运行情况：

* [Pomodoro Do](https://itunes.apple.com/app/id533469911?mt=8)： 这是我自己开发的应用，运行正常，但是因为不是iOS7 SDK打包，所以在UI上使用了之前系统的，问题是导航栏Tint颜色丢失，导致很难看，需要尽快更新。
* Facebook：因为使用了图片自定义导航栏，而没有直接使用系统提供的材质，所以没什么问题。
* 面包旅行：直接Crash，无法打开，原因未知。



这次UI大改可以说是一次对敏捷开发的检验，原来的应用（特别是拟物化用得比较重的应用）虽然也能运行，但是很多UI自定义的地方需要更改不说，还容易让用户产生一种“来到了另一个世界”的感觉，同时可以看到也有部分应用无法运行。而对于苹果的封闭系统和只升不降的特性，开发者以及其应用必须要尽快适应这个新系统，这对于迭代快速，还在继续维护的应用来说会是一个机会。相信谁先能适应新的UI，谁就将在iOS7上占到先机。

#### UIKit的力学模型（UIKit Dynamics）
新增了`UIDynamicItem`委托，用来为UIView制定力学模型行为，当然其他任何对象都能通过实现这组接口来定义动力学行为，只不过在UIKit中可能应用最多。所谓动力学行为，是指将现实世界的我们常见的力学行为或者特性引入到UI中，比如重力等。通过实现UIDynamicItem，UIKit现在支持如下行为：

* UIAttachmentBehavior 连接两个实现了UIDynamicItem的物体（以下简称动力物体），一个物体移动时，另一个跟随移动
* UICollisionBehavior 指定边界，使两个动力物体可以进行碰撞
* UIGravityBehavior 顾名思义，为动力物体增加重力模拟
* UIPushBehavior 为动力物体施加持续的力
* UISnapBehavior 为动力物体指定一个附着点，想象一下类似挂一幅画在图钉上的感觉

如果有开发游戏的童鞋可能会觉得这些很多都是做游戏时候的需求，一种box2d之类的2D物理引擎的既视感跃然而出。没错的亲，动态UI，加上之后要介绍的Sprite Kit，极大的扩展了使用UIKit进行游戏开发的可能性。另外要注意UIDynamicItem不仅适用于UIKit，任何对象都可以实现接口来获得动态物体的一些特性，所以说用来做一些3D的或者其他奇怪有趣的事情也不是没有可能。如果觉得Cocos2D+box2d这样的组合使用起来不方便的话，现在动态UIKit+SpriteKit给出了新的选择。

### 游戏方面

iOS7 SDK极大加强了直接使用iOS SDK制作和分发游戏的体验，最主要的是引入了专门的游戏制作框架。

#### Sprite Kit Framework
这是个人认为iOS7 SDK最大的亮点，也是最重要的部分，iOS SDK终于有自己的精灵系统了。Sprite Kit Framework使用硬件加速的动画系统来表现2D和2.5D的游戏，它提供了制作游戏所需要的大部分的工具，包括图像渲染，动画系统，声音播放以及图像模拟的物理引擎。可以说这个框架是iOS SDK自带了一个较完备的2D游戏引擎，力图让开发者专注于更高层的实现和内容。和大多数游戏引擎一样，Sprite Kit内的内容都按照场景（Scene）来分开组织，一个场景可以包括贴图对象，视频，形状，粒子效果甚至是CoreImage滤镜等等。相对于现有的2D引擎来说，由于Sprite Kit是在系统层级进行的优化，渲染时间等都由框架决定，因此应该会有比较高的效率。

另外，Xcode还提供了创建粒子系统和贴图Atlas的工具。使用Xcode来管理粒子效果和贴图atlas，可以迅速在Sprite Kit中反应出来。

#### Game Controller Framework
为Made-for-iPhone/iPod/iPad (MFi) game controller设计的硬件的对应的框架，可以让用户用来连接和控制专门的游戏硬件。参考WWDC 2013开场视频中开始的赛车演示。现在想到的是，也许这货不仅可以用于游戏…或者苹果之后会扩展其应用，因为使用普及率很高的iPhone作为物联网的入口，似乎会是很有前途的事情。

#### GameCenter改进
GameCenter一直是苹果的败笔...虽然每年都在改进，但是一直没看到大的起色。今年也不例外，都是些小改动，不提也罢。

### 多任务强化

* 经常需要下载新内容的应用现在可以通过设置`UIBackgroundModes`为`fetch`来实现后台下载内容了，需要在AppDelegate里实现`setMinimumBackgroundFetchInterval:`以及`application:performFetchWithCompletionHandler: `来处理完成的下载，这个为后台运行代码提供了又一种选择。不过考虑到Apple如果继续严格审核的话，可能只有杂志报刊类应用能够取得这个权限吧。另外需要注意开发者仅只能指定一个最小间隔，最后下没下估计就得看系统娘的心情了。
* 同样是后台下载，以前只能推送提醒用户进入应用下载，现在可以接到推送并在后台下载。UIBackgroundModes设为remote-notification，并实现`application:didReceiveRemoteNotification:fetchCompletionHandler:`

为后台下载，开发者必须使用一个新的类`NSURLSession`，其实就是在NSURLConnection上加了个后台处理，使用类似，API十分简单，不再赘述。

### AirDrop
这个是iOS7的重头新功能，用户可以用它来分享照片，文档，链接，或者其他数据给附近的设备。但是不需要特别的实现，被集成在了标准的UIActivityViewController里，并没有单独的API提供。数据的话，可以通过实现UIActivityItemSource接口后进行发送。大概苹果也不愿意看到超出他们控制的文件分享功能吧，毕竟这和iOS设计的初衷不一样。如果你不使用UIActivityViewController的话，可能是无法在应用里实装AirDrop功能了。

### 地图
Apple在继续在地图应用上的探索，MapKit的改进也乏善可陈。我一直相信地图类应用的瓶颈一定在于数据，但是对于数据源的建立并不是一年两年能够完成的。Google在这一块凭借自己的搜索引擎有着得天独厚的优势，苹果还差的很远很远。看看有哪些新东西吧：

* MKMapCamera，可以将一个MKMapCamera对象添加到地图上，在指明位置，角度和方向后将呈现3D的样子…大概可以想象成一个数字版的Google街景..
* MKDirections 获取Apple提供的基于方向的路径，然后可以用来将路径绘制在自己的应用中。这可能对一些小的地图服务提供商产生冲击，但是还是那句话，地图是一个数据的世界，在拥有完备数据之前，Apple不是Google的对手。这个状况至少会持续好几年（也有可能是永远）。
* MKGeodesicPolyline 创建一个随地球曲率的线，并附加到地图上，完成一些视觉效果。
* MKMapSnapshotter 使用其拍摄基于地图的照片，也许各类签到类应用会用到
* 改变了overlay物件的渲染方式

### Inter-App Audio 应用间的音频
AudioUnit框架中加入了在同一台设备不同应用之间发送MIDI指令和传送音频的能力。比如在一个应用中使用AudioUnit录音，然后在另一个应用中打开以处理等。在音源应用中声明一个AURemoteIO实例来标为Inter-App可用，在目标应用中使用新的发现接口来发现并获取音频。
想法很好，也算是在应用内共享迈出了一步，不过我对现在使用AudioUnit这样的低层级框架的应用数量表示不乐观。也许今后会有一些为更高层级设计的共享API提供给开发者使用。毕竟要从AudioUnit开始处理音频对于大多数开发者来说并不是一件很容易的事情。

### 点对点连接 Peer-to-Peer Connectivity
可以看成是AirDrop不能直接使用的补偿，代价是需要自己实现。MultipeerConnectivity框架可以用来发现和连接附近的设备，并传输数据，而这一切并不需要有网络连接。可以看到Apple逐渐在文件共享方面一步步放开限制，但是当然所有这些都还是被限制在sandbox里的。

### Store Kit Framework
Store Kit在内购方面采用了新的订单系统，这将可以实现对订单的本机验证。这是一次对应内购破解和有可能验证失败导致内购失败的更新，苹果希望藉此减少内购的实现流程，减少出错，同时遏制内购破解泛滥。前者可能没有问题，但是后者的话，因为objc的动态特性，决定了只要有越狱存在，内购破解也是早晚的事情。不过这一点确实方便了没有能力架设验证服务器的小开发者，这方面来说还是很好的。

### 最后
当然还有一些其他小改动，包括MessageUI里添加了附件按钮，Xcode开始支持模块了等等。完整的iOS7新特性列表可以在[这里](https://developer.apple.com/library/prerelease/ios/releasenotes/General/WhatsNewIniOS/Articles/iOS7.html#//apple_ref/doc/uid/TP40013162-SW1)找到（暂时应该也需要开发者账号）。最后一个好消息是，苹果放慢了废弃API的速度，这个版本并没有特别重要的API被标为Deprecated，Cheers。
-->

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[iOS中使用blend改变图片颜色]]></title>
    <link href="http://onevcat.com/2013/04/using-blending-in-ios/"/>
    <updated>2013-04-29T16:30:00+09:00</updated>
    <id>http://onevcat.com/2013/04/using-blending-in-ios</id>
    <content type="html"><![CDATA[<p><img src="http://img.onevcat.com/2013/blend_title.png" alt="使用Blend处理图片颜色" /></p>

<p>最近对<code>Core Animation</code>和<code>Core Graphics</code>的内容东西比较感兴趣，自己之前也在这块相对薄弱，趁此机会也想补习一下这块的内容，所以之后几篇可能都会是对CA和CG学习的记录的文章。</p>

<p>在应用里一个很常见的需求是主题变换：同样的图标，同样的素材，但是需要按照用户喜爱变为不同的颜色。在iOS5和6的SDK里部分标准控件引入了<code>tintColor</code>，来满足个性化界面的需求，但是Apple在这方面还远远做的不够。一是现在用默认控件根本难以做出界面优秀的应用，二是<code>tintColor</code>所覆盖的并不够全面，在很多情况下开发者都无法使用其来完成个性化定义。解决办法是什么？最简单当然是拜托设计师大大出图，想要蓝色主题？那好，开PS盖个蓝色图层出一套蓝色的UI；又想加粉色UI，那好，再出一套粉色的图然后导入Xcode。代码上的话根据颜色需求使用image-blue或者image-pink这样的名字来加载图片。</p>

<p>如果有一丁点重构的意识，就会知道这不是一个很好的解决方案。工程中存在大量的冗余和重复（就算你要狡辩这些图片颜色不同不算重复，你也会在内心里知道这种狡辩是多么无力），这是非常致命的。想象一下如果你有10套主题界面，先不论应用的体积会膨胀到多少，光是想做一点修改就会痛苦万分，比如希望改一下某个按钮的形状，很好，设计师大大请重复地修改10遍，并出10套UI，然后一系列的重命名，文件移动和导入…一场灾难。</p>

<p>当然有其他办法，因为说白了就是tint不同的颜色到图片上而已，如果我们能实现改变UIImage的颜色，那我们就只需要一套UI，然后用代码来改变UI的颜色就可以了，生活有木有一下光明起来呀。嗯，让我们先从一张图片开始吧～下面是一张带有alpha通道的图片，原始颜色是纯的灰色（当然什么颜色都可以，只不过我这个人现在暂时比较喜欢灰色而已）。</p>

<!-- more -->


<p><img src="http://img.onevcat.com/2013/blend_origin.png" alt="要处理的原图" /></p>

<p>我们将用blending给这张图片加上另一个纯色作为tint，并保持原来的alpha通道。用Core Graphics来做的话，大概的想法很直接：</p>

<ol>
<li>创建一个上下文用以画新的图片</li>
<li>将新的tintColor设置为填充颜色</li>
<li>将原图片画在创建的上下文中，并用新的填充色着色（注意保持alpha通道）</li>
<li>从当前上下文中取得图片并返回</li>
</ol>


<p>最麻烦的部分可能就是保持alpha通道了。<a href="https://developer.apple.com/library/ios/#documentation/UIKit/Reference/UIImage_Class/Reference/Reference.html">UIImage的文档</a>中提供了使用blend绘图的方法<code>drawInRect:blendMode:alpha:</code>，<code>rect</code>和<code>alpha</code>都没什么问题，但是<code>blendMode</code>是个啥玩意儿啊…继续看文档，关于<a href="https://developer.apple.com/library/ios/#documentation/GraphicsImaging/Reference/CGContext/Reference/reference.html#//apple_ref/doc/c_ref/CGBlendMode"><code>CGBlendMode</code>的文档</a>，里面有一大堆看不懂的枚举值，比如这样：</p>

<p><code>
kCGBlendModeDestinationOver
R = S*(1 - Da) + D
Available in iOS 2.0 and later.
Declared in CGContext.h.
</code></p>

<p>完全不懂..直接看之后的Discussion部分：</p>

<blockquote><p>The blend mode constants introduced in OS X v10.5 represent the Porter-Duff blend modes. The symbols in the equations for these blend modes are:<br/>
R is the premultiplied result<br/>
S is the source color, and includes alpha<br/>
D is the destination color, and includes alpha<br/>
Ra, Sa, and Da are the alpha components of R, S, and D</p></blockquote>

<p>原来如此，R表示结果，S表示包含alpha的原色，D表示包含alpha的目标色，Ra，Sa和Da分别是三个的alpha。明白了这些以后，就可以开始寻找我们所需要的blend模式了。相信你可以和我一样，很快找到这个模式：</p>

<p><code>
kCGBlendModeDestinationIn
R = D*Sa
Available in iOS 2.0 and later.
Declared in CGContext.h.
</code></p>

<p>结果 = 目标色和原色透明度的加成，看起来正式所需要的。啦啦啦，还等什么呢，开始动手实现看看对不对吧～</p>

<p>为了以后使用方便，当然是祭出Category，先创建一个UIImage的类别：</p>

<p>```objc
//  UIImage+Tint.h</p>

<h1>import &lt;UIKit/UIKit.h></h1>

<p>@interface UIImage (Tint)</p>

<ul>
<li>(UIImage <em>) imageWithTintColor:(UIColor </em>)tintColor;</li>
</ul>


<p>@end
```
暂时先这样，当然我们也可以创建一个类方法直接完成从bundle读取图片然后加tintColor，但是很多时候并不如上面一个实例方法方便（比如想要从非bundle的地方获取图片），这个问题之后再说。那么就按照之前设想的步骤来实现吧：</p>

<p>```objc
//  UIImage+Tint.m</p>

<h1>import "UIImage+Tint.h"</h1>

<p>@implementation UIImage (Tint)
- (UIImage <em>) imageWithTintColor:(UIColor </em>)tintColor
{</p>

<pre><code>//We want to keep alpha, set opaque to NO; Use 0.0f for scale to use the scale factor of the device’s main screen.
UIGraphicsBeginImageContextWithOptions(self.size, NO, 0.0f);
[tintColor setFill];
CGRect bounds = CGRectMake(0, 0, self.size.width, self.size.height);
UIRectFill(bounds);

//Draw the tinted image in context
[self drawInRect:bounds blendMode:kCGBlendModeDestinationIn alpha:1.0f];

UIImage *tintedImage = UIGraphicsGetImageFromCurrentImageContext();
UIGraphicsEndImageContext();

return tintedImage;
</code></pre>

<p>}
@end
```</p>

<p>简单明了，没有任何难点。测试之：<code>[[UIImage imageNamed:@"image"] imageWithTintColor:[UIColor orangeColor]];</code>，得到的结果为：</p>

<p><img src="http://img.onevcat.com/2013/blend_1.png" alt="使用kCGBlendModeDestinationIn模式的结果" /></p>

<p>嗯...怎么说呢，虽然tintColor的颜色是变了，但是总觉得怪怪的。仔细对比一下就会发现，原来灰色图里星星和周围的灰度渐变到了橙色的图里好像都消失了：星星整个变成了橙色，周围的一圈漂亮的光晕也没有了，这是神马情况啊…这种图能交差的话那算见鬼了，会被设计和产品打死的吧。对于无渐变的纯色图的图来说直接用上面的方法是没问题的，但是现在除了Metro的大色块以外哪里无灰度渐变的设计啊…检查一下使用的blend，<code>R = D * Sa</code>，恍然大悟，我们虽然保留了原色的透明度，但是却把它的所有的灰度信息弄丢了。怎么办？继续刨<code>CGBlendMode</code>的文档吧，那么多blend模式应该总有我们需要的。功夫不负有心人，<code>kCGBlendModeOverlay</code>一副嗷嗷待选的样子：</p>

<p><code>
kCGBlendModeOverlay
Either multiplies or screens the source image samples with the background image samples, depending on the background color. The result is to overlay the existing image samples while preserving the highlights and shadows of the background. The background color mixes with the source image to reflect the lightness or darkness of the background.
Available in iOS 2.0 and later.
Declared in CGContext.h.
</code></p>

<p>kCGBlendModeOverlay可以保持背景色的明暗，也就是灰度信息，听起来正是我们需要的。加入到声明中，并且添加相应的实现( 顺便重构一下原来的代码 :) )：</p>

<p>```objc
//  UIImage+Tint.h</p>

<h1>import &lt;UIKit/UIKit.h></h1>

<p>@interface UIImage (Tint)</p>

<ul>
<li>(UIImage <em>) imageWithTintColor:(UIColor </em>)tintColor;</li>
<li>(UIImage <em>) imageWithGradientTintColor:(UIColor </em>)tintColor;</li>
</ul>


<p>@end
```</p>

<p>```objc
//  UIImage+Tint.m</p>

<h1>import "UIImage+Tint.h"</h1>

<p>@implementation UIImage (Tint)
- (UIImage <em>) imageWithTintColor:(UIColor </em>)tintColor
{</p>

<pre><code>return [self imageWithTintColor:tintColor blendMode:kCGBlendModeDestinationIn];
</code></pre>

<p>}</p>

<ul>
<li><p>(UIImage <em>) imageWithGradientTintColor:(UIColor </em>)tintColor
{
  return [self imageWithTintColor:tintColor blendMode:kCGBlendModeOverlay];
}</p></li>
<li><p>(UIImage <em>) imageWithTintColor:(UIColor </em>)tintColor blendMode:(CGBlendMode)blendMode
{
  //We want to keep alpha, set opaque to NO; Use 0.0f for scale to use the scale factor of the device’s main screen.
  UIGraphicsBeginImageContextWithOptions(self.size, NO, 0.0f);
  [tintColor setFill];
  CGRect bounds = CGRectMake(0, 0, self.size.width, self.size.height);
  UIRectFill(bounds);</p>

<p>  //Draw the tinted image in context
  [self drawInRect:bounds blendMode:blendMode alpha:1.0f];</p>

<p>  UIImage *tintedImage = UIGraphicsGetImageFromCurrentImageContext();
  UIGraphicsEndImageContext();</p>

<p>  return tintedImage;
}</p></li>
</ul>


<p>@end
```</p>

<p>完成，测试之…好吧，好尴尬，虽然颜色和周围的光这次对了，但是透明度又没了啊魂淡..一点不奇怪啊，因为<code>kCGBlendModeOverlay</code>本来就没承诺给你保留原图的透明度的说。</p>

<p><img src="http://img.onevcat.com/2013/blend_2.png" alt="使用kCGBlendModeOverlay模式的结果" /></p>

<p>那么..既然我们用<code>kCGBlendModeOverlay</code>能保留灰度信息，用<code>kCGBlendModeDestinationIn</code>能保留透明度信息，那就两个blendMode都用不就完事儿了么～尝试之，如果在blend绘图时不是<code>kCGBlendModeDestinationIn</code>模式的话，则再用<code>kCGBlendModeDestinationIn</code>画一次：</p>

<p>```objc
//  UIImage+Tint.m</p>

<h1>import "UIImage+Tint.h"</h1>

<p>@implementation UIImage (Tint)
- (UIImage <em>) imageWithTintColor:(UIColor </em>)tintColor
{</p>

<pre><code>return [self imageWithTintColor:tintColor blendMode:kCGBlendModeDestinationIn];
</code></pre>

<p>}</p>

<ul>
<li><p>(UIImage <em>) imageWithGradientTintColor:(UIColor </em>)tintColor
{
  return [self imageWithTintColor:tintColor blendMode:kCGBlendModeOverlay];
}</p></li>
<li><p>(UIImage <em>) imageWithTintColor:(UIColor </em>)tintColor blendMode:(CGBlendMode)blendMode
{
  //We want to keep alpha, set opaque to NO; Use 0.0f for scale to use the scale factor of the device’s main screen.
  UIGraphicsBeginImageContextWithOptions(self.size, NO, 0.0f);
  [tintColor setFill];
  CGRect bounds = CGRectMake(0, 0, self.size.width, self.size.height);
  UIRectFill(bounds);</p>

<p>  //Draw the tinted image in context
  [self drawInRect:bounds blendMode:blendMode alpha:1.0f];</p>

<p>  if (blendMode != kCGBlendModeDestinationIn) {</p>

<pre><code>  [self drawInRect:bounds blendMode:kCGBlendModeDestinationIn alpha:1.0f];
</code></pre>

<p>  }</p>

<p>  UIImage *tintedImage = UIGraphicsGetImageFromCurrentImageContext();
  UIGraphicsEndImageContext();</p>

<p>  return tintedImage;
}</p></li>
</ul>


<p>@end
```</p>

<p>结果如下：</p>

<p><img src="http://img.onevcat.com/2013/blend_3.png" alt="使用kCGBlendModeOverlay和kCGBlendModeDestinationIn模式的结果" /></p>

<p>已经很完美了，这样的话只要在代码里设定一下颜色，我们就能够很轻易地使用同样一套UI，将其blend为需要的颜色，来实现素材的重用了。唯一需要注意的是，因为每次使用<code>UIImage+Tint</code>的方法绘图时，都使用了CG的绘制方法，这就意味着每次调用都会是用到CPU的Offscreen drawing，大量使用的话可能导致性能的问题（主要对于iPhone 3GS或之前的设备，可能同时处理大量这样的绘制调用的能力会有不足）。关于CA和CG的性能的问题，打算在之后用一篇文章来介绍一下。对于这里的<code>UIImage+Tint</code>的实现，可以写一套缓存的机制，来确保大量重复的元素只在load的时候blend一次，之后将其缓存在内存中以快速读取。当然这是一个权衡的问题，在时间和空间中做出正确的平衡和选择，也正是程序设计的乐趣所在。</p>

<p>这篇文章中作为示例的工程和UIImage+Tint可以在<a href="https://github.com/onevcat/VVImageTint">Github</a>上找到，您可以随意玩弄..我相信也会是个来研究每种blend的特性的好机会～</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[两个人一起记账吧～ Our Money]]></title>
    <link href="http://onevcat.com/2013/04/our-money-app/"/>
    <updated>2013-04-06T11:54:00+09:00</updated>
    <id>http://onevcat.com/2013/04/our-money-app</id>
    <content type="html"><![CDATA[<p><a href="https://itunes.apple.com/cn/app/our-money/id562520527?ls=1&amp;mt=8"><img src="http://img.onevcat.com/2013/ourmoney-banner.jpg" alt="image" /></a></p>

<p>Our Money是一款能够协助多人在云端记账的iOS应用，可以帮助您简单地记录和整理日常开销，您可以邀请您的朋友和家人与您一起记账，免去每日汇报总结之苦。</p>

<ul>
<li><a href="https://itunes.apple.com/cn/app/our-money/id562520527?ls=1&amp;mt=8">App Store地址</a></li>
<li><a href="http://ourmoney.onevcat.com">Our Money app的首页</a></li>
</ul>


<p>大概但凡从按月领生活费开始花钱以来，都会兴起记账的念头，至于是否能够坚持，就各凭本事了。说到自己，则是多次付诸行动，然后不了了之。从一开始记在小本本上自己用计算器加加减减，到建个Excel文档自动求和，再到手机上的记账应用，时代在进步，咱的手段也在进步，却总还觉得没有找到最合适的工具。尤其是用手机记账以来，有的软件，每次对非得给一笔开销定义出两层的分类，让我头疼不已，家庭小帐非得整成个公司帐簿，改动标签也颇为麻烦；有的软件，记录条目倒是简单，但其他诸如统计等功能却也一起被简化了。不过，最让我郁闷的是，记账总成为我一个人的事情，谁让是用我的手机在记呢。</p>

<p>现在，终于等到了一款操作简单但是功能齐全，尤其是，<strong>可以多人共同记账的应用</strong>。这款叫做Our Money 的应用，最大的亮点当然就在于“Our”。它可以实现多人一起记账，只要人手一个应用，就可以和家人一起记录家庭开销，和朋友一起整理出游费用，不同的帐本可以选择和不同的人分享，每个人都能参与，条目更新实时同步，再不用一个人负责所有的帐目。</p>

<p>好啦，废话不多说，让我们一起来体验一下这个软件吧。<a href="https://itunes.apple.com/cn/app/our-money/id562520527?ls=1&amp;mt=8">下载应用</a>并打开，用邮箱注册用户，就可以开始记账啦。请记住你的邮箱是你邀请别人或者别人邀请你共同记账的标识哦～</p>

<!-- more -->




<br>


<p><img src="http://img.onevcat.com/2013/1-ourmoney-login.png" alt="OurMoney的注册登陆界面" />
Our Money的主界面相当简洁，最上方列出列表名称，收入（预算）、支出、结余也一目了然，条目的时间、分类、备注都一目了然。那么其他其他内容被藏在哪里呢？左边一拉，当前列表的按月总计；右边一拉，列表编辑，数据统计，就是这么简单～</p>

<br>


<p><img src="http://img.onevcat.com/2013/2-ourmoney-month.png" alt="按月份统计收入和开销" /></p>

<br>


<p><img src="http://img.onevcat.com/2013/3-ourmoney-stat.png" alt="按项目和用户的统计" />
首先我们新建一个列表， 在右边的界面下拉一下，就可以新建自己的列表了。选中的列表下方能够修改列表名称或者删除，中间的邀请就是重头戏啦，输入希望一同记账的朋友的邮箱，他就可以收到邀请并加入你的列表。当邀请了朋友或家人加入列表后，列表信息中就会显示多人同为列表用户。当然，在记账时随时可以邀请新的用户加入。</p>

<br>


<p><img src="http://img.onevcat.com/2013/4-ourmoney-invite.png" alt="邀请别人加入特定列表一起记账" />
选定刚才新建的列表，回到主界面，随便记下一点东西，在同一列表中的用户将通过推送（如果允许的话）收到您更改了列表的消息。而对方打开应用时，马上就可以同步地看到您所记录的信息，这便于双方更迅速地各自完成记账，免去了回家后苦苦思索或者汇总的麻烦，确实十分方便。</p>

<br>


<p><img src="http://img.onevcat.com/2013/5-ourmoney-push.png" alt="家人或朋友记账后，立即可以收到系统提醒" /></p>

<p>记错了，找不到修改的地方怎么办？点一下，记录被选中，下面就出现了编辑或者删除的选项，还可以分享条目到社交网络，秀一下收到的礼物什么的哦～</p>

<p>在消费和记账时难免会出现没有网络的尴尬时候，这时候Our Money还能正常工作么？当然，Our Money具有完善的离线模式处理，没有网络时照常使用，当之后连上网络的时候会自动为您完成所有同步，完全不用自己操心。</p>

<br>


<p><img src="http://img.onevcat.com/2013/6-ourmoney-offline.png" alt="Our Money方便的离线模式" /></p>

<p>总的来说Our Money是一款功能强大但又简单高效的记账软件，其云端记账和共同记账的理念很符合当今多人记账的需求。从今天开始就和家人朋友用Our Money一起记账吧～</p>

<p>您可以从<a href="https://itunes.apple.com/cn/app/our-money/id562520527?ls=1&amp;mt=8">App Store中下载Our Money</a>，还可以进一步通过应用内的赠送系统将您的记账和心得分享给家人朋友。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MGTwitterEngine中Twitter API 1.1的使用]]></title>
    <link href="http://onevcat.com/2013/03/mgtwitterengine%E4%B8%ADtwitter-api-1-1%E7%9A%84%E4%BD%BF%E7%94%A8/"/>
    <updated>2013-03-24T00:00:00+09:00</updated>
    <id>http://onevcat.com/2013/03/mgtwitterengine中twitter-api-1-1的使用</id>
    <content type="html"><![CDATA[<p>在iOS5中使用Twitter framework或者在iOS6中使用Social framework来完成Twitter的集成是非常简单和轻松的，但是如果应用要针对iOS5之前的系统版本，那么就不能使用iOS提供的框架了。一个比较常见也是使用最广泛的选择是<a href="https://github.com/mattgemmell/MGTwitterEngine">MGTwitterEngine</a>，比如<a href="http://www.onevcat.com/showcase/pomodoro_do/">PomodoroDo</a>选择使用的就是该框架。</p>

<p>但是今天在对PomodoroDo作更新的时候，发现Twitter的分享无法使用了，在查阅Twitter文档说明之后，发现这是Twitter采用了新版API的原因。默认状况下MGTwitterEngine采用的是v1版的API，并且使用XML的版本进行请求，而在1.1中，将<a href="https://dev.twitter.com/docs/api/1.1/overview#JSON_support_only">只有JSON方式的API可以使用</a>。v1.0版本的API已经于2013年3月5日被完全废弃，因此想要继续使用MGTwitterEngine来适配iOS5之前的Twitter集成需求，就需要将MGTwitterEngine的请求改为JSON方式。MGTwitterEngine也考虑到了这一点，但是因为时间比较古老了，MGTwitterEngine使用了YAJL来作为JSON的Wrapper，因此还需要将YAJL集成进来。下午的时候尝试了一会儿，成功地让MGTwitterEngine用上了1.1的Twitter API，为了以防之后别人或是自己可能遇到同样的问题，将更新的方法在此留底备忘。</p>

<ol>
<li><p>导入YAJL Framework</p>

<ul>
<li>YAJL的OC实现，从<a href="https://github.com/gabriel/yajl-objc/download">该地址下载该框架</a>。(2013年3月24日的最新版本为YAJL 0.3.1 for iOS)</li>
<li>解压下载得到的zip，将解压后的YAJLiOS.framework加入项目工程</li>
<li>在Xcode的Build Setting里在Other Linker Flags中添加-ObjC和-all_load标记</li>
</ul>
</li>
<li><p>加入MGTwitterEngine的JSON相关代码</p>

<ul>
<li>从<a href="https://github.com/mattgemmell/MGTwitterEngine">MGTwitterEngine的页面</a>down下该项目。当然如果有新版或者有别的branch可以用的话更省事儿，但是鉴于MGTwitterEngine现在的活跃度来说估计可能性不大，所以还是乖乖自己更新吧。</li>
<li>解开下载的zip，用Xcode打开MGTwitterEngine.xcodeproj工程文件，将其中Twitter YAJL Parsers组下的所有文件copy到自己的项目中。</li>
</ul>
</li>
<li><p>YAJL头文件集成</p>

<ul>
<li>接下来是C和OC接口头文件的导入，从下面下载YAJL库：<a href="https://github.com/thinglabs/yajl-objc">https://github.com/thinglabs/yajl-objc</a></li>
<li>在下载得到的文件夹中，寻找并将以下h文件拷贝到自己的工程中：

<ul>
<li>yajl_common.h</li>
<li>yajl_gen.h</li>
<li>yajl_parse.h</li>
<li>NSObject+YAJL.h</li>
<li>YAJL.h</li>
<li>YAJLDocument.h</li>
<li>YAJLGen.h</li>
<li>YAJLParser.h</li>
</ul>
</li>
</ul>
</li>
<li><p>最后是在MGTwitterEngine设定为使用v1.1 API以及JSON方式请求</p></li>
</ol>


<p>在MGTwitterEngine.m中，将对应代码修改为以下：</p>

<p>```objc</p>

<h1>define USE_LIBXML 0</h1>

<h1>define TWITTER_DOMAIN @"api.twitter.com/1.1"</h1>

<p>```</p>

<p>在MGbTwitader.h，启用YAJL</p>

<p>```objc</p>

<h1>define define YAJL_AVAILABLE 1</h1>

<p>```</p>

<p>本文参考：</p>

<p><a href="https://github.com/mattgemmell/MGTwitterEngine/issues/107">MGTwitterEngine issues 107</a></p>

<p><a href="http://damienh.org/2009/06/20/setting-up-mgtwitterengine-with-yajl-106-for-iphone-development/">http://damienh.org/2009/06/20/setting-up-mgtwitterengine-with-yajl-106-for-iphone-development/</a></p>
]]></content>
  </entry>
  
</feed>
