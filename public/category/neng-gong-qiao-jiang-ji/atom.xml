<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: 能工巧匠集 | OneV's Den]]></title>
  <link href="http://onevcat.com/category/neng-gong-qiao-jiang-ji/atom.xml" rel="self"/>
  <link href="http://onevcat.com/"/>
  <updated>2013-08-31T22:20:55+09:00</updated>
  <id>http://onevcat.com/</id>
  <author>
    <name><![CDATA[onevcat]]></name>
    <email><![CDATA[onev@onevcat.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[猫都能学会的Unity3D Shader入门指南（二）]]></title>
    <link href="http://onevcat.com/2013/08/shader-tutorial-2/"/>
    <updated>2013-08-31T22:18:00+09:00</updated>
    <id>http://onevcat.com/2013/08/shader-tutorial-2</id>
    <content type="html"><![CDATA[<p><img src="http://img.onevcat.com/2013/shader-tutorial2-light.jpg" alt="Unity Shader教程" /></p>

<h2>关于本系列</h2>

<p>这是Unity3D Shader入门指南系列的第二篇，本系列面向的对象是新接触Shader开发的Unity3D使用者，因为我本身自己也是Shader初学者，因此可能会存在错误或者疏漏，如果您在Shader开发上有所心得，很欢迎并恳请您指出文中纰漏，我会尽快改正。在<a href="http://onevcat.com/2013/07/shader-tutorial-1/">之前的开篇</a>中介绍了一些Shader的基本知识，包括ShaderLab的基本结构和语法，以及简单逐句地讲解了一个基本的shader。在具有这些基础知识后，阅读简单的shader应该不会有太大问题，在继续教程之前简单阅读一下Unity的<a href="http://docs.unity3d.com/Documentation/Components/SL-SurfaceShaderExamples.html">Surface Shader Example</a>，以检验您是否掌握了上一节的内容。如果您对阅读大部分示例Shader并没有太大问题，可以正确地指出Shader的结构，声明和使用的话，就说明您已经准备好继续阅读本节的内容了。</p>

<h2>法线贴图(Normal Mapping)</h2>

<p>法线贴图是凸凹贴图(Bump mapping)的一种常见应用，简单说就是在不增加模型多边形数量的前提下，通过渲染暗部和亮部的不同颜色深度，来为原来的贴图和模型增加视觉细节和真实效果。简单原理是在普通的贴图的基础上，再另外提供一张对应原来贴图的，可以表示渲染浓淡的贴图。通过将这张附加的表示表面凸凹的贴图的因素于实际的原贴图进行运算后，可以得到新的细节更加丰富富有立体感的渲染效果。在本节中，我们将首先实现一个法线贴图的Shader，然后对Unity Shader的光照模型进行一些讨论，并实现一个自定义的光照模型。最后再通过更改shader模拟一个石头上的积雪效果，并对模型顶点进行一些修改使积雪效果看起来比较真实。在本节结束的时候，我们就会有一个比较强大的可以满足一些真实开发工作时可用的shader了，而且更重要的是，我们将会掌握它是如何被创造出来的。</p>

<!--more-->


<p>关于法线贴图的效果图，可以对比看看下面。模型面数为500，左侧只使用了简单的Diffuse着色，右侧使用了法线贴图。比较两张图片不难发现，使用了法线贴图的石头在暗部和亮部都有着更好的表现。整体来说，凸凹感比Diffuse的结果增强许多，石头看起来更真实也更具有质感。</p>

<p><img src="http://img.onevcat.com/2013/shader-tutorial2-compare.jpg" alt="image" /></p>

<p>本节中需要用到的上面的素材可以<a href="http://vdisk.weibo.com/s/y-NNpUsxhYhZI">在这里下载</a>，其中包括上面的石块的模型，一张贴图以及对应的法线贴图。将下载的package导入到工程中，并新建一个material，使用简单的Diffuse的Shader（比如上一节我们实现的），再加上一个合适的平行光光源，就可以得到我们左图的效果。另外，本节以及以后都会涉及到一些Unity内建的Shader的内容，比如一些标准常用函数和常量定义等，相关内容可以在Unity的内建Shader中找到，内建Shader可以在<a href="http://unity3d.com/unity/download/archive">Unity下载页面</a>的版本右侧找到。</p>

<p>接下来我们实现法线贴图。在实现之前，我们先简单地稍微多了解一些法线贴图的基本知识。大多数法线图一般都和下面的图类似，是一张以蓝紫色为主的图。这张法线图其实是一张RGB贴图，其中红，绿，蓝三个通道分别表示由高度图转换而来的该点的法线指向：Nx、Ny、Nz。在其中绝大部分点的法线都指向z方向，因此图更偏向于蓝色。在shader进行处理时，我们将光照与该点的法线值进行点积后即可得到在该光线下应有的明暗特性，再将其应用到原图上，即可反应在一定光照环境下物体的凹凸关系了。关于法向贴图的更多信息，可以参考<a href="http://en.wikipedia.org/wiki/Normal_mapping">wiki上的相关条目</a>。</p>

<p><img src="http://img.onevcat.com/2013/shader-tutorial2-normal.jpg" alt="一张典型的法线图" /></p>

<p>回到正题，我们现在考虑的主要是Shader入门，而不是图像学的原理。再上一节我们写的Shader的基础上稍微做一些修改，就可以得到适应并完成法线贴图渲染的新Shader。新加入的部分进行了编号并在之后进行说明。</p>

<p>```glsl
Shader "Custom/Normal Mapping" {</p>

<pre><code>Properties {
    _MainTex ("Base (RGB)", 2D) = "white" {}

    //1
    _Bump ("Bump", 2D) = "bump" {}
}
SubShader {
    Tags { "RenderType"="Opaque" }
    LOD 200

    CGPROGRAM
    #pragma surface surf Lambert

    sampler2D _MainTex;

    //2
    sampler2D _Bump;                

    struct Input {
        float2 uv_MainTex;

        //3
        float2 uv_Bump;
    };

    void surf (Input IN, inout SurfaceOutput o) {
        half4 c = tex2D (_MainTex, IN.uv_MainTex);

        //4
        o.Normal = UnpackNormal(tex2D(_Bump, IN.uv_Bump);

        o.Albedo = c.rgb;
        o.Alpha = c.a;
    }
    ENDCG
} 
FallBack "Diffuse"
</code></pre>

<p>}
```</p>

<ol>
<li>声明并加入一个显示名称为<code>Bump</code>的贴图，用于放置法线图</li>
<li>为了能够在CG程序中使用这张贴图，必须加入一个sample，希望你还记得～</li>
<li>获取Bump的uv信息作为输入</li>
<li>从法线图中提取法线信息，并将其赋予相应点的输出的Normal属性。<code>UnpackNormal</code>是定义在UnityCG.cginc文件中的方法，这个文件中包含了一系列常用的CG变量以及方法。<code>UnpackNormal</code>接受一个fixed4的输入，并将其转换为所对应的法线值（fixed3）。在解包得到这个值之后，将其赋给输出的Normal，就可以参与到光线运算中完成接下来的渲染工作了。</li>
</ol>


<p>现在保存并且编译这个Shader，创建新的material并使用这个shader，将石头的材质贴图和法线图分别拖放到Base和Bump里，再将其应用到石头模型上，应该就可以看到右侧图的效果了。</p>

<h2>光照模型</h2>

<p>在我们之前的看到的Shader中（其实也就上一节的基本diffuse和这里的normal mapping），都只使用了Lambert的光照模型（#pragma surface surf Lambert），这是一个很经典的漫反射模型，光强与入射光的方向和反射点处表面法向夹角的余弦成正比。关于Lambert和漫反射的一些详细的计算和推论，可以参看wiki（<a href="http://en.wikipedia.org/wiki/Lambertian_reflectance">Lambert</a>，<a href="http://en.wikipedia.org/wiki/Diffuse_reflection">漫反射</a>）或者其他地方的介绍。一句话的简单解释就是一个点的反射光强是和该点的法线向量和入射光向量和强度和夹角有关系的，其结果就是这两个向量的点积。既然已经知道了光照计算的原理，我们先来看看如何实现一个自己的光照模型吧。</p>

<p>在刚才的Shader上进行如下修改。</p>

<ul>
<li>首先将原来的<code>#pragma</code>行改为这样</li>
</ul>


<p>```glsl</p>

<h1>pragma surface surf CustomDiffuse</h1>

<p>```</p>

<ul>
<li>然后在SubShader块中添加如下代码</li>
</ul>


<p>```glsl
inline float4 LightingCustomDiffuse (SurfaceOutput s, fixed3 lightDir, fixed atten) {</p>

<pre><code>float difLight = max(0, dot (s.Normal, lightDir));
float4 col;
col.rgb = s.Albedo * _LightColor0.rgb * (difLight * atten * 2);
col.a = s.Alpha;
return col;
</code></pre>

<p>}
```</p>

<ul>
<li>最后保存，回到Unity。Shader将编译，如果一切正常，你将不会看到新的shader和之前的在材质表现上有任何不同。但是事实上我们现在的shader已经与Unity内建的diffuse光照模型撇清了关系，而在使用我们自己设定的光照模型了。</li>
</ul>


<p>喵的，这些代码都干了些什么！相信你一定会有这样的疑惑...没问题，没有疑惑的话那就不叫初学了，还是一行行讲来。首先正像我们上一篇所说，<code>#pragma</code>语句在这里声明了接下来的Shader的类型，计算调用的方法名，以及指定光照模型。在之前我们一直指定Lambert为光照模型，而现在我们将其换为了CustomDiffuse。</p>

<p>接下来添加的代码是计算光照的实现。shader中对于方法的名称有着比较严格的约定，想要创建一个光照模型，首先要做的是按照规则声明一个光照计算的函数名字，即<code>Lighting&lt;Your Chosen Name&gt;</code>。对于我们的光照模型CustomDiffuse，其计算函数的名称自然就是<code>LightingCustomDiffuse</code>了。光照模型的计算是在surf方法的表面颜色之后，根据输入的光照条件来对原来的颜色在这种光照下的表现进行计算，最后输出新的颜色值给渲染单元完成在屏幕的绘制。</p>

<p>也许你已经猜到了，我们之前用的Lambert光照模型是不是也有一个名字叫LightingLambert的光照计算函数呢？Bingo。在Unity的内建Shader中，有一个Lighting.cginc文件，里面就包含了LightingLambert的实现。也许你也注意到了，我们所实现的LightingCustomDiffuse的内容现在和Unity内建中的LightingLambert是完全一样的，这也就是使用新的shader的原来视觉上没有区别的原因，因为实现确实是完全一样的。</p>

<p>首先来看输入量，<code>SurfaceOutput s</code>这个就是经过表面计算函数surf处理后的输出，我们讲对其上的点根据光线进行处理，<code>fixed3 lightDir</code>是光线的方向，<code>fixed atten</code>表示光衰减的系数。在计算光照的代码中，我们先将输入的s的法线值（在Normal mapping中的话这个值已经是法线图中的对应量了）和输入光线进行点积（dot函数是CG中内置的数学函数，希望你还记得，可以<a href="http://http.developer.nvidia.com/CgTutorial/cg_tutorial_chapter05.html">参考这里</a>）。点积的结果在-1至1之间，这个值越大表示法线与光线间夹角越小，这个点也就应该越亮。之后使用max来将这个系数结果限制在0到1之间，是为了避免负数情况的存在而导致最终计算的颜色变为负数，输出一团黑，一般来说这是我们不愿意看到的。接下来我们将surf输出的颜色与光线的颜色<code>_LightColor0.rgb</code>（由Unity根据场景中的光源得到的，它在Lighting.cginc中有声明）进行乘积，然后再与刚才计算的光强系数和输入的衰减系数相乘，最后得到在这个光线下的颜色输出（关于difLight * atten * 2中为什么有个乘2，这是一个历史遗留问题，主要是为了进行一些光强补偿，可以参见<a href="http://forum.unity3d.com/threads/94711-Why-(atten-*-2">这里的讨论</a>)）。</p>

<p>在了解了基本实现方式之后，我们可以看看做一些修改玩玩儿。最简单的比如将这个Lambert模型改亮一些，比如换成Half Lambert模型。Half Lambert是由Valve创造的可以使物体在低光线条件下增亮的技术，最早被用于半条命（Half Life）中以避免在低光下物体的走形。简单说就是把光强系数先取一半，然后在加0.5，代码如下：</p>

<p>```glsl
inline float4 LightingCustomDiffuse (SurfaceOutput s, fixed3 lightDir, fixed atten) {</p>

<pre><code>float difLight = dot (s.Normal, lightDir);
float hLambert = difLight * 0.5 + 0.5;
float4 col;
col.rgb = s.Albedo * _LightColor0.rgb * (hLambert * atten * 2);
col.a = s.Alpha;
return col;
</code></pre>

<p>}
```</p>

<p>这样一来，原来光强0的点，现在对应的值变为了0.5，而原来是1的地方现在将保持为1。也就是说模型贴图的暗部被增强变亮了，而亮部基本保持和原来一样，防止过曝。使用Half Lambert前后的效果图如下，注意最右侧石头下方的阴影处细节更加明显了，而这一切都只是视觉效果的改变，不涉及任何贴图和模型的变化。</p>

<p><img src="http://img.onevcat.com/2013/shader-toturial-hl.jpg" alt="Half Lambert下发现贴图的表现" /></p>

<h2>表面贴图的追加效果</h2>

<p>OK，对于光线和自定义光照模型的讨论暂时到此为止，因为如果展开的话这将会一个庞大的图形学和经典光学的话题了。我们回到Shader，并且一起实现一些激动人心的效果吧。比如，在你的游戏场景中有一幕是雪地场景，而你希望做一些石头上白雪皑皑的覆盖效果，应该怎么办呢？难道让你可爱的3D设计师再去出一套覆雪的贴图然后使用新的贴图？当然不，不是不能，而是不该。因为新的贴图不仅会增大项目的资源包体积，更会增大之后修改和维护的难度，想想要是有好多石头需要实现同样的覆雪效果，或者是要随着游戏时间堆积的雪逐渐变多的话，你应该怎么办？难道让设计师再把所有的石头贴图都盖上雪，然后再按照雪的厚度出5套不同的贴图么？相信我，他们会疯的。</p>

<p>于是，我们考虑用Shader来完成这件工作吧！先考虑下我们需要什么，积雪效果的话，我们需要积雪等级（用来表示积雪量），雪的颜色，以及积雪的方向。基本思路和实现自定义光照模型类似，通过计算原图的点在世界坐标中的法线方向与积雪方向的点积，如果大于设定的积雪等级的阈值的话则表示这个方向与积雪方向是一致的，其上是可以积雪的，显示雪的颜色，否则使用原贴图的颜色。废话不再多说，上代码，在上面的Shader的基础上，更改Properties里的内容为</p>

<p>```glsl
Properties {</p>

<pre><code>_MainTex ("Base (RGB)", 2D) = "white" {}
_Bump ("Bump", 2D) = "bump" {}
_Snow ("Snow Level", Range(0,1) ) = 0
_SnowColor ("Snow Color", Color) = (1.0,1.0,1.0,1.0)
_SnowDirection ("Snow Direction", Vector) = (0,1,0)
</code></pre>

<p>}
```</p>

<p>没有太多值得说的，唯一要提一下的是_SnowDirection设定的默认值为(0,1,0)，这表示我们希望雪是垂直落下的。对应地，在CG程序中对这些变量进行声明：</p>

<p><code>glsl
sampler2D _MainTex;
sampler2D _Bump;
float _Snow;
float4 _SnowColor;
float4 _SnowDirection;
</code></p>

<p>接下来改变Input的内容：</p>

<p>```glsl
struct Input {</p>

<pre><code>float2 uv_MainTex;
float2 uv_Bump;
float3 worldNormal; INTERNAL_DATA
</code></pre>

<p>};
```</p>

<p>相对于上面的Shader输入来说，加入了一个<code>float3 worldNormal; INTERNAL_DATA</code>，如果SurfaceOutput中设定了Normal值的话，通过worldNormal可以获取当前点在世界中的法线值。详细的解说可以参见<a href="http://docs.unity3d.com/Documentation/Components/SL-SurfaceShaders.html">Unity的Shader文档</a>。接下来可以改变surf函数，实装积雪效果了。</p>

<p>```glsl
void surf (Input IN, inout SurfaceOutput o) {</p>

<pre><code>half4 c = tex2D (_MainTex, IN.uv_MainTex);
o.Normal = UnpackNormal(tex2D(_Bump, IN.uv_Bump));

if (dot(WorldNormalVector(IN, o.Normal), _SnowDirection.xyz) &gt; lerp(1,-1,_Snow)) {
    o.Albedo = _SnowColor.rgb;
} else {
    o.Albedo = c.rgb;
}

o.Alpha = c.a;
</code></pre>

<p>}
```</p>

<p>和上面相比，加入了一个if…else…的判断。首先看这个条件的不等式的左侧，我们对雪的方向和和输入点的世界法线方向进行点积。<code>WorldNormalVector</code>通过输入的点及这个点的法线值，来计算它在世界坐标中的方向；右侧的lerp函数相信只要对插值有概念的同学都不难理解：当<em>Snow取最小值1时，这个函数将返回1，而</em>Snow取最大值时，返回-1。这样我们就可以通过设定<em>Snow的值来控制积雪的阈值，要是积雪等级</em>Snow是0时，不等式左侧不可能大于右侧，因此完全没有积雪；相反要是_Snow取最大值1时，由于左侧必定大于-1，所以全模型积雪。而随着取中间值的变化，积雪的情况便会有所不同。</p>

<p>应用这个Shader，并且适当地调节一下积雪等级和颜色，可以得到如下右边的效果。</p>

<p><img src="http://img.onevcat.com/2013/shader-tutorial2-snow.jpg" alt="添加了积雪效果的Shader" /></p>

<h2>更改顶点模型</h2>

<p>到现在位置，我们还仅指是在原贴图上进行操作，不管是用法线图使模型看起来凸凹有致，还是加上积雪，所有的计算和颜色的输出都只是“障眼法”，并没有对模型有任何实质的改动。但是对于积雪效果来说，实际上积雪是附加到石头上面，而不应当简单替换掉原来的颜色。但是具体实施起来，最简单的办法还是直接替换颜色，但是我们可以稍微变更一下模型，使原来的模型在积雪的方向稍微变大一些，这样来达到一种雪是附加到石头上的效果。</p>

<p>我们继续修改之前的Shader，首先我们需要告诉surface shadow我们要改变模型的顶点。首先将#param行改为</p>

<p><code>#pragma surface surf CustomDiffuse vertex:vert</code></p>

<p>这告诉Shader我们想要改变模型顶点，并且我们会写一个叫做<code>vert</code>的函数来改变顶点。接下来我们再添加一个参数，在Properties中声明一个<code>_SnowDepth</code>变量，表示积雪的厚度，当然我们也需要在CG段中进行声明：</p>

<p>```glsl
//In Properties{…}
_SnowDepth ("Snow Depth", Range(0,0.3)) = 0.1</p>

<p>//In CG declare
float _SnowDepth;
```</p>

<p>接下来实现vert方法，和之前积雪的运算其实比较类似，判断点积大小来决定是否需要扩大模型以及确定模型扩大的方向。在CG段中加入以下vert方法</p>

<p>```glsl
void vert (inout appdata_full v) {</p>

<pre><code>float4 sn = mul(transpose(_Object2World) , _SnowDirection);
if(dot(v.normal, sn.xyz) &gt;= lerp(1,-1, (_Snow * 2) / 3)) {
    v.vertex.xyz += (sn.xyz + v.normal) * _SnowDepth * _Snow;
}
</code></pre>

<p>}
```</p>

<p>和surf的原理差不多，系统会输入一个当前的顶点的值，我们根据需要计算并填上新的值作为返回即可。上面第一行中使用<code>transpose</code>方法输出原矩阵的转置矩阵，在这里_Object2World是Unity ShaderLab的内建值，它表示将当前模型转换到世界坐标中的矩阵，将其与积雪方向做矩阵乘积得到积雪方向在物体的世界空间中的投影（把积雪方向转换到世界坐标中）。之后我们计算了这个世界坐标中实际的积雪方向和当前点的法线值的点积，并将结果与使用积雪等级的2/3进行比较lerp后的阈值比较。这样，当前点如果和积雪方向一致，并且积雪较为完整的话，将改变该点的模型顶点高度。</p>

<p>加入模型更改前后的效果对比如下图，加入模型调整的右图表现要更为丰满真实。</p>

<p><img src="http://img.onevcat.com/2013/shader-tutorial2-snow-vert.jpg" alt="image" /></p>

<p>这节就到这里吧。本节中实现的Shader可以<a href="https://gist.github.com/onevcat/6396814">在这里找到完整版本</a>进行参考，希望大家周末愉快～</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[猫都能学会的Unity3D Shader入门指南（一）]]></title>
    <link href="http://onevcat.com/2013/07/shader-tutorial-1/"/>
    <updated>2013-07-23T23:14:00+09:00</updated>
    <id>http://onevcat.com/2013/07/shader-tutorial-1</id>
    <content type="html"><![CDATA[<p><img src="http://img.onevcat.com/2013/shader-tutorial-banner.jpg" alt="Unity Shader教程" /></p>

<h2>动机</h2>

<p>自己使用Unity3D也有一段时间了，但是很多时候是流于表面，更多地是把这个引擎简单地用作脚本控制，而对更深入一些的层次几乎没有了解。虽然说Unity引擎设计的初衷就是创建简单的不需要开发者操心的谁都能用的3D引擎，但是只是肤浅的使用，可能是无法达到随心所欲的境地的，因此，这种状况必须改变！从哪里开始呢，貌似有句话叫做会写Shader的都是高手，于是，想大概看看从Shader开始能不能使自己到达的层次能再深入一些吧，再于是，有了这个系列（希望我能坚持写完它，虽然应该会拖个半年左右）。</p>

<p>Unity3D的所有渲染工作都离不开着色器（Shader），如果你和我一样最近开始对Shader编程比较感兴趣的话，可能你和我有着同样的困惑：如何开始？Unity3D提供了一些Shader的手册和文档（比如<a href="http://docs.unity3d.com/Documentation/Manual/Shaders.html">这里</a>，<a href="http://docs.unity3d.com/Documentation/Components/Built-inShaderGuide.html">这里</a>和<a href="http://docs.unity3d.com/Documentation/Components/SL-Reference.html">这里</a>），但是一来内容比较分散，二来学习阶梯稍微陡峭了些。这对于像我这样之前完全没有接触过有关内容的新人来说是相当不友好的。国内外虽然也有一些Shader的介绍和心得，但是也同样存在内容分散的问题，很多教程前一章就只介绍了基本概念，接下来马上就搬出一个超复杂的例子，对于很多基本的用法并没有解释。也许对于Shader熟练使用的开发者来说是没有问题，但是我相信像我这样的入门者也并不在少数。在多方寻觅无果后，我觉得有必要写一份教程，来以一个入门者的角度介绍一些Shader开发的基本步骤。其实与其说是教程，倒不如说是一份自我总结，希望能够帮到有需要的人。</p>

<p>所以，本“教程”的对象是</p>

<ul>
<li>总的来说是新接触Shader开发的人：也许你知道什么是Shader，也会使用别人的Shader，但是仅限于知道一些基本的内建Shader名字，从来没有打开它们查看其源码。</li>
<li>想要更多了解Shader和有需求要进行Shader开发的开发者，但是之前并没有Shader开发的经验。</li>
</ul>


<p>当然，因为我本身在Shader开发方面也是一个不折不扣的大菜鸟，本文很多内容也只是在自己的理解加上一些可能不太靠谱的求证和总结。本文中的示例应该会有更好的方式来实现，因此您是高手并且恰巧路过的话，如果有好的方式来实现某些内容，恳请您不吝留下评论，我会对本文进行不断更新和维护。</p>

<!--more-->


<h2>一些基本概念</h2>

<h3>Shader和Material</h3>

<p>如果是进行3D游戏开发的话，想必您对着两个词不会陌生。Shader（着色器）实际上就是一小段程序，它负责将输入的Mesh（网格）以指定的方式和输入的贴图或者颜色等组合作用，然后输出。绘图单元可以依据这个输出来将图像绘制到屏幕上。输入的贴图或者颜色等，加上对应的Shader，以及对Shader的特定的参数设置，将这些内容（Shader及输入参数）打包存储在一起，得到的就是一个Material（材质）。之后，我们便可以将材质赋予合适的renderer（渲染器）来进行渲染（输出）了。</p>

<p>所以说Shader并没有什么特别神奇的，它只是一段规定好输入（颜色，贴图等）和输出（渲染器能够读懂的点和颜色的对应关系）的程序。而Shader开发者要做的就是根据输入，进行计算变换，产生输出而已。</p>

<p>Shader大体上可以分为两类，简单来说</p>

<ul>
<li>表面着色器（Surface Shader） - 为你做了大部分的工作，只需要简单的技巧即可实现很多不错的效果。类比卡片机，上手以后不太需要很多努力就能拍出不错的效果。</li>
<li>片段着色器（Fragment Shader） - 可以做的事情更多，但是也比较难写。使用片段着色器的主要目的是可以在比较低的层级上进行更复杂（或者针对目标设备更高效）的开发。</li>
</ul>


<p>因为是入门文章，所以之后的介绍将主要集中在表面着色器上。</p>

<h3>Shader程序的基本结构</h3>

<p>因为着色器代码可以说专用性非常强，因此人为地规定了它的基本结构。一个普通的着色器的结构应该是这样的：
<img src="http://img.onevcat.com/2013/shader-structure.png" alt="一段Shader程序的结构" /></p>

<p>首先是一些属性定义，用来指定这段代码将有哪些输入。接下来是一个或者多个的子着色器，在实际运行中，哪一个子着色器被使用是由运行的平台所决定的。子着色器是代码的主体，每一个子着色器中包含一个或者多个的Pass。在计算着色时，平台先选择最优先可以使用的着色器，然后依次运行其中的Pass，然后得到输出的结果。最后指定一个回滚，用来处理所有Subshader都不能运行的情况（比如目标设备实在太老，所有Subshader中都有其不支持的特性）。</p>

<p>需要提前说明的是，在实际进行表面着色器的开发时，我们将直接在Subshader这个层次上写代码，系统将把我们的代码编译成若干个合适的Pass。废话到此为止，下面让我们真正实际进入Shader的世界吧。</p>

<h2>Hello Shader</h2>

<p>百行文档不如一个实例，下面给出一段简单的Shader代码，然后根据代码来验证下上面说到的结构和阐述一些基本的Shader语法。因为本文是针对Unity3D来写Shader的，所以也使用Unity3D来演示吧。首先，新建一个Shader，可以在Project面板中找到，Create，选择Shader，然后将其命名为<code>Diffuse Texture</code>：</p>

<p><img src="http://img.onevcat.com/2013/shader-create-in-unity.png" alt="在Unity3D中新建一个Shader" /></p>

<p>随便用个文本编辑器打开刚才新建的Shader：</p>

<p>```glsl
Shader "Custom/Diffuse Texture" {</p>

<pre><code>Properties {
    _MainTex ("Base (RGB)", 2D) = "white" {}
}
SubShader {
    Tags { "RenderType"="Opaque" }
    LOD 200

    CGPROGRAM
    #pragma surface surf Lambert

    sampler2D _MainTex;

    struct Input {
        float2 uv_MainTex;
    };

    void surf (Input IN, inout SurfaceOutput o) {
        half4 c = tex2D (_MainTex, IN.uv_MainTex);
        o.Albedo = c.rgb;
        o.Alpha = c.a;
    }
    ENDCG
} 
FallBack "Diffuse"
</code></pre>

<p>}</p>

<p>```</p>

<p>如果您之前没怎么看过Shader代码的话，估计细节上会看不太懂。但是有了上面基本结构的介绍，您应该可以识别出这个Shader的构成，比如一个Properties部分，一个SubShader，以及一个FallBack。另外，第一行只是这个Shader的声明并为其指定了一个名字，比如我们的实例Shader，你可以在材质面板选择Shader时在对应的位置找到这个Shader。</p>

<p><img src="http://img.onevcat.com/2013/shader-select.png" alt="在Unity3D中找到刚才新建的Shader" /></p>

<p><strong>接下来我们讲逐句讲解这个Shader，以期明了每一个语句的意义。</strong></p>

<h3>属性</h3>

<p>在<code>Properties{}</code>中定义着色器属性，在这里定义的属性将被作为输入提供给所有的子着色器。每一条属性的定义的语法是这样的：</p>

<p><code>_Name("Display Name", type) = defaultValue[{options}]</code></p>

<ul>
<li>_Name - 属性的名字，简单说就是变量名，在之后整个Shader代码中将使用这个名字来获取该属性的内容</li>
<li>Display Name - 这个字符串将显示在Unity的材质编辑器中作为Shader的使用者可读的内容</li>
<li>type - 这个属性的类型，可能的type所表示的内容有以下几种：

<ul>
<li>Color - 一种颜色，由RGBA（红绿蓝和透明度）四个量来定义；</li>
<li>2D - 一张2的阶数大小（256，512之类）的贴图。这张贴图将在采样后被转为对应基于模型UV的每个像素的颜色，最终被显示出来；</li>
<li>Rect - 一个非2阶数大小的贴图；</li>
<li>Cube - 即Cube map texture（立方体纹理），简单说就是6张有联系的2D贴图的组合，主要用来做反射效果（比如天空盒和动态反射），也会被转换为对应点的采样；</li>
<li>Range(min, max) - 一个介于最小值和最大值之间的浮点数，一般用来当作调整Shader某些特性的参数（比如透明度渲染的截止值可以是从0至1的值等）；</li>
<li>Float - 任意一个浮点数；</li>
<li>Vector - 一个四维数；</li>
</ul>
</li>
<li>defaultValue 定义了这个属性的默认值，通过输入一个符合格式的默认值来指定对应属性的初始值（某些效果可能需要某些特定的参数值来达到需要的效果，虽然这些值可以在之后在进行调整，但是如果默认就指定为想要的值的话就省去了一个个调整的时间，方便很多）。

<ul>
<li>Color - 以0～1定义的rgba颜色，比如(1,1,1,1)；</li>
<li>2D/Rect/Cube - 对于贴图来说，默认值可以为一个代表默认tint颜色的字符串，可以是空字符串或者"white","black","gray","bump"中的一个</li>
<li>Float，Range - 某个指定的浮点数</li>
<li>Vector - 一个4维数，写为 (x,y,z,w)</li>
</ul>
</li>
<li>另外还有一个{option}，它只对2D，Rect或者Cube贴图有关，在写输入时我们最少要在贴图之后写一对什么都不含的空白的{}，当我们需要打开特定选项时可以把其写在这对花括号内。如果需要同时打开多个选项，可以使用空白分隔。可能的选择有ObjectLinear, EyeLinear, SphereMap, CubeReflect, CubeNormal中的一个，这些都是OpenGL中TexGen的模式，具体的留到后面有机会再说。</li>
</ul>


<p>所以，一组属性的申明看起来也许会是这个样子的</p>

<p><code>glsl
//Define a color with a default value of semi-transparent blue
_MainColor ("Main Color", Color) = (0,0,1,0.5)
//Define a texture with a default of white
_Texture ("Texture", 2D) = "white" {}
</code></p>

<p>现在看懂上面那段Shader（以及其他所有Shader）的Properties部分应该不会有任何问题了。接下来就是SubShader部分了。</p>

<h3>Tags</h3>

<p>表面着色器可以被若干的标签（tags）所修饰，而硬件将通过判定这些标签来决定什么时候调用该着色器。比如我们的例子中SubShader的第一句</p>

<p><code>Tags { "RenderType"="Opaque" }</code></p>

<p>告诉了系统应该在渲染非透明物体时调用我们。Unity定义了一些列这样的渲染过程，与RenderType是Opaque相对应的显而易见的是<code>"RenderType" = "Transparent"</code>，表示渲染含有透明效果的物体时调用。在这里Tags其实暗示了你的Shader输出的是什么，如果输出中都是非透明物体，那写在Opaque里；如果想渲染透明或者半透明的像素，那应该写在Transparent中。</p>

<p>另外比较有用的标签还有<code>"IgnoreProjector"="True"</code>（不被<a href="http://docs.unity3d.com/Documentation/Components/class-Projector.html">Projectors</a>影响），<code>"ForceNoShadowCasting"="True"</code>（从不产生阴影）以及<code>"Queue"="xxx"</code>（指定渲染顺序队列）。这里想要着重说一下的是Queue这个标签，如果你使用Unity做过一些透明和不透明物体的混合的话，很可能已经遇到过不透明物体无法呈现在透明物体之后的情况。这种情况很可能是由于Shader的渲染顺序不正确导致的。Queue指定了物体的渲染顺序，预定义的Queue有：</p>

<ul>
<li>Background - 最早被调用的渲染，用来渲染天空盒或者背景</li>
<li>Geometry - 这是默认值，用来渲染非透明物体（普通情况下，场景中的绝大多数物体应该是非透明的）</li>
<li>AlphaTest - 用来渲染经过<a href="http://docs.unity3d.com/Documentation/Components/SL-AlphaTest.html">Alpha Test</a>的像素，单独为AlphaTest设定一个Queue是出于对效率的考虑</li>
<li>Transparent - 以从后往前的顺序渲染透明物体</li>
<li>Overlay - 用来渲染叠加的效果，是渲染的最后阶段（比如镜头光晕等特效）</li>
</ul>


<p>这些预定义的值本质上是一组定义整数，Background = 1000， Geometry = 2000, AlphaTest = 2450， Transparent = 3000，最后Overlay = 4000。在我们实际设置Queue值时，不仅能使用上面的几个预定义值，我们也可以指定自己的Queue值，写成类似这样：<code>"Queue"="Transparent+100"</code>，表示一个在Transparent之后100的Queue上进行调用。通过调整Queue值，我们可以确保某些物体一定在另一些物体之前或者之后渲染，这个技巧有时候很有用处。</p>

<h3>LOD</h3>

<p>LOD很简单，它是Level of Detail的缩写，在这里例子里我们指定了其为200（其实这是Unity的内建Diffuse着色器的设定值）。这个数值决定了我们能用什么样的Shader。在Unity的Quality Settings中我们可以设定允许的最大LOD，当设定的LOD小于SubShader所指定的LOD时，这个SubShader将不可用。Unity内建Shader定义了一组LOD的数值，我们在实现自己的Shader的时候可以将其作为参考来设定自己的LOD数值，这样在之后调整根据设备图形性能来调整画质时可以进行比较精确的控制。</p>

<ul>
<li>VertexLit及其系列 = 100</li>
<li>Decal, Reflective VertexLit = 150</li>
<li>Diffuse = 200</li>
<li>Diffuse Detail, Reflective Bumped Unlit, Reflective Bumped VertexLit = 250</li>
<li>Bumped, Specular = 300</li>
<li>Bumped Specular = 400</li>
<li>Parallax = 500</li>
<li>Parallax Specular = 600</li>
</ul>


<h3>Shader本体</h3>

<p>前面杂项说完了，终于可以开始看看最主要的部分了，也就是将输入转变为输出的代码部分。为了方便看，请容许我把上面的SubShader的主题部分抄写一遍</p>

<p>```glsl
CGPROGRAM</p>

<h1>pragma surface surf Lambert</h1>

<p>sampler2D _MainTex;</p>

<p>struct Input {</p>

<pre><code>float2 uv_MainTex;
</code></pre>

<p>};</p>

<p>void surf (Input IN, inout SurfaceOutput o) {</p>

<pre><code>half4 c = tex2D (_MainTex, IN.uv_MainTex);
o.Albedo = c.rgb;
o.Alpha = c.a;
</code></pre>

<p>}
ENDCG
```</p>

<p>还是逐行来看，首先是CGPROGRAM。这是一个开始标记，表明从这里开始是一段CG程序（我们在写Unity的Shader时用的是Cg/HLSL语言）。最后一行的ENDCG与CGPROGRAM是对应的，表明CG程序到此结束。</p>

<p>接下来是是一个编译指令：<code>#pragma surface surf Lambert</code>，它声明了我们要写一个表面Shader，并指定了光照模型。它的写法是这样的</p>

<p><code>#pragma surface surfaceFunction lightModel [optionalparams]</code></p>

<ul>
<li>surface - 声明的是一个表面着色器</li>
<li>surfaceFunction - 着色器代码的方法的名字</li>
<li>lightModel - 使用的光照模型。</li>
</ul>


<p>所以在我们的例子中，我们声明了一个表面着色器，实际的代码在surf函数中（在下面能找到该函数），使用Lambert（也就是普通的diffuse）作为光照模型。</p>

<p>接下来一句<code>sampler2D _MainTex;</code>，sampler2D是个啥？其实在CG中，sampler2D就是和texture所绑定的一个数据容器接口。等等..这个说法还是太复杂了，简单理解的话，所谓加载以后的texture（贴图）说白了不过是一块内存存储的，使用了RGB（也许还有A）通道，且每个通道8bits的数据。而具体地想知道像素与坐标的对应关系，以及获取这些数据，我们总不能一次一次去自己计算内存地址或者偏移，因此可以通过sampler2D来对贴图进行操作。更简单地理解，sampler2D就是GLSL中的2D贴图的类型，相应的，还有sampler1D，sampler3D，samplerCube等等格式。</p>

<p>解释通了sampler2D是什么之后，还需要解释下为什么在这里需要一句对<code>_MainTex</code>的声明，之前我们不是已经在<code>Properties</code>里声明过它是贴图了么。答案是我们用来实例的这个shader其实是由两个相对独立的块组成的，外层的属性声明，回滚等等是Unity可以直接使用和编译的ShaderLab；而现在我们是在<code>CGPROGRAM...ENDCG</code>这样一个代码块中，这是一段CG程序。对于这段CG程序，要想访问在<code>Properties</code>中所定义的变量的话，<strong>必须使用和之前变量相同的名字进行声明</strong>。于是其实<code>sampler2D _MainTex;</code>做的事情就是再次声明并链接了_MainTex，使得接下来的CG程序能够使用这个变量。</p>

<p>终于可以继续了。接下来是一个struct结构体。相信大家对于结构体已经很熟悉了，我们先跳过之，直接看下面的的surf函数。上面的#pragma段已经指出了我们的着色器代码的方法的名字叫做surf，那没跑儿了，就是这段代码是我们的着色器的工作核心。我们已经说过不止一次，着色器就是给定了输入，然后给出输出进行着色的代码。CG规定了声明为表面着色器的方法（就是我们这里的surf）的参数类型和名字，因此我们没有权利决定surf的输入输出参数的类型，只能按照规定写。这个规定就是第一个参数是一个Input结构，第二个参数是一个inout的SurfaceOutput结构。</p>

<p>它们分别是什么呢？Input其实是需要我们去定义的结构，这给我们提供了一个机会，可以把所需要参与计算的数据都放到这个Input结构中，传入surf函数使用；SurfaceOutput是已经定义好了里面类型输出结构，但是一开始的时候内容暂时是空白的，我们需要向里面填写输出，这样就可以完成着色了。先仔细看看INPUT吧，现在可以跳回来看上面定义的INPUT结构体了：</p>

<p>```glsl
struct Input {</p>

<pre><code>float2 uv_MainTex;
</code></pre>

<p>};
```</p>

<p>作为输入的结构体必须命名为Input，这个结构体中定义了一个float2的变量…你没看错我也没打错，就是float2，表示浮点数的float后面紧跟一个数字2，这又是什么意思呢？其实没什么魔法，float和vec都可以在之后加入一个2到4的数字，来表示被打包在一起的2到4个同类型数。比如下面的这些定义：</p>

<p><code>
//Define a 2d vector variable
vec2 coordinate;
//Define a color variable
float4 color;
//Multiply out a color
float3 multipliedColor = color.rgb * coordinate.x;
</code></p>

<p>在访问这些值时，我们即可以只使用名称来获得整组值，也可以使用下标的方式（比如.xyzw，.rgba或它们的部分比如.x等等）来获得某个值。在这个例子里，我们声明了一个叫做<code>uv_MainTex</code>的包含两个浮点数的变量。</p>

<p>如果你对3D开发稍有耳闻的话，一定不会对uv这两个字母感到陌生。UV mapping的作用是将一个2D贴图上的点按照一定规则映射到3D模型上，是3D渲染中最常见的一种顶点处理手段。在CG程序中，我们有这样的约定，在一个贴图变量（在我们例子中是<code>_MainTex</code>）之前加上uv两个字母，就代表提取它的uv值（其实就是两个代表贴图上点的二维坐标 ）。我们之后就可以在surf程序中直接通过访问uv_MainTex来取得这张贴图当前需要计算的点的坐标值了。</p>

<p>如果你坚持看到这里了，那要恭喜你，因为离最后成功读完一个Shader只有一步之遥。我们回到surf函数，它的两有参数，第一个是Input，我们已经明白了：在计算输出时Shader会多次调用surf函数，每次给入一个贴图上的点坐标，来计算输出。第二个参数是一个可写的SurfaceOutput，SurfaceOutput是预定义的输出结构，我们的surf函数的目标就是根据输入把这个输出结构填上。SurfaceOutput结构体的定义如下</p>

<p>```glsl
struct SurfaceOutput {</p>

<pre><code>half3 Albedo;     //像素的颜色
half3 Normal;     //像素的法向值
half3 Emission;   //像素的发散颜色
half Specular;    //像素的镜面高光
half Gloss;       //像素的发光强度
half Alpha;       //像素的透明度
</code></pre>

<p>};
```</p>

<p>这里的half和我们常见float与double类似，都表示浮点数，只不过精度不一样。也许你很熟悉单精度浮点数（float或者single）和双精度浮点数（double），这里的half指的是半精度浮点数，精度最低，运算性能相对比高精度浮点数高一些，因此被大量使用。</p>

<p>在例子中，我们做的事情非常简单：</p>

<p><code>glsl
half4 c = tex2D (_MainTex, IN.uv_MainTex);
o.Albedo = c.rgb;
o.Alpha = c.a;
</code></p>

<p>这里用到了一个<code>tex2d</code>函数，这是CG程序中用来在一张贴图中对一个点进行采样的方法，返回一个float4。这里对_MainTex在输入点上进行了采样，并将其颜色的rbg值赋予了输出的像素颜色，将a值赋予透明度。于是，着色器就明白了应当怎样工作：即找到贴图上对应的uv点，直接使用颜色信息来进行着色，over。</p>

<h2>接下来...</h2>

<p>我想现在你已经能读懂一些最简单的Shader了，接下来我推荐的是参考Unity的<a href="http://docs.unity3d.com/Documentation/Components/SL-SurfaceShaderExamples.html">Surface Shader Examples</a>多接触一些各种各样的基本Shader。在这篇教程的基础上，配合一些google的工作，完全看懂这个shader示例页面应该不成问题。如果能做到无压力看懂，那说明你已经有良好的基础可以前进到Shader的更深的层次了（也许等不到我的下一篇教程就可以自己开始动手写些效果了）；如果暂时还是有困难，那也没有关系，Shader学习绝对是一个渐进的过程，因为有很多约定和常用技巧，多积累和实践自然会进步并掌握。</p>

<p>在接下来的教程里，打算通过介绍一些实际例子以及从基础开始实际逐步动手实现一个复杂一点的例子，让我们能看到shader在真正使用中的威力。我希望能尽快写完这个系列，但是无奈时间确实有限，所以我也不知道什么时候能出炉...写好的时候我会更改这段内容并指向新的文章。您要是担心错过的话，也可以使用<a href="http://eepurl.com/wNSkj">邮件订阅</a>或者<a href="http://onevcat.com/atom.xml">订阅本站的rss</a>(虽然Google Reader已经关了- -)。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[开发者所需要知道的iOS7 SDK新特性]]></title>
    <link href="http://onevcat.com/2013/06/developer-should-know-about-ios7/"/>
    <updated>2013-06-11T07:43:00+09:00</updated>
    <id>http://onevcat.com/2013/06/developer-should-know-about-ios7</id>
    <content type="html"><![CDATA[<p><img src="http://img.onevcat.com/2013/ios-7-logo.png" alt="iOS 7" /></p>

<p>春风又绿加州岸，物是人非又一年。WWDC 2013 keynote落下帷幕，新的iOS开发旅程也由此开启。在iOS7界面重大变革的背后，开发者们需要知道的又有哪些呢。同去年一样，我会先简单纵览地介绍iOS7中我个人认为开发者需要着重关注和学习的内容，之后再陆续对自己感兴趣章节进行探索。计划继承类似<a href="http://onevcat.com/2012/06/%E5%BC%80%E5%8F%91%E8%80%85%E6%89%80%E9%9C%80%E8%A6%81%E7%9F%A5%E9%81%93%E7%9A%84ios6-sdk%E6%96%B0%E7%89%B9%E6%80%A7/">WWDC2012的笔记</a>的形式，希望对国内开发者有所帮助。</p>

<p>相关笔记整理如下：（为了避免不必要的麻烦，所有笔记暂时就不在blog上公开全文发布了，先作为自己的笔记进行记录备份。只给出链接，之后NDA解除后再一并公开发吧..不过本站从源代码开始都是架在github上的..所以..嗯不多说了..）</p>

<ul>
<li>总览 <a href="http://onevcat.com/2013/06/developer-should-know-about-ios7/">开发者所需要知道的iOS7 SDK新特性</a></li>
<li>工具 <a href="http://onevcat.com/">WWDC2013笔记 Xcode5和ObjC新特性</a> http://onevcat.com/2013/06/new-in-xcode5-and-objc/</li>
<li>UIKit动力学 <a href="http://onevcat.com/">WWDC2013笔记 UIKit力学模型入门</a> http://onevcat.com/2013/06/uikit-dynamics-started/</li>
<li>SpriteKit入门 <a href="http://onevcat.com/">WWDC2013笔记 SpriteKit快速入门和新时代iOS游戏开发指南</a> http://onevcat.com/2013/06/sprite-kit-start/</li>
</ul>


<!--
---

### UI相关
#### 全新UI设计
iOS7最大的变化莫过于UI设计，也许你会说UI设计“这是设计师大大们应该关注的事情，不关开发者的事，我们只需要替换图片就行了”。那你就错了。UI的变化必然带来使用习惯和方式的转变，如何运用iOS7的UI，如何是自己的应用更切合新的系统，都是需要考虑的事情。另外值得注意的是，使用iOS7 SDK（现在只有Xcode5预览版提供）打包的应用在iOS7上运行时将会自动使用iOS7的新界面，所以原有应用可能需要对新界面进行重大调整。具体的iOS7中所使用的UI元素的人际交互界面文档，可以从[这里](https://developer.apple.com/library/prerelease/ios/design/index.html#//apple_ref/doc/uid/TP40013289)找到（应该是需要开发者账号才能看）。

简单总结来说，以现在上手体验看来新的UI变化改进有如下几点：

* 状态栏，导航栏和应用实际展示内容不再界限：系统自带的应用都不再区分状态栏和navigation bar，而是用统一的颜色力求简洁。这也算是一种趋势。
* BarItem的按钮全部文字化：这点做的相当坚决，所有的导航和工具条按钮都取消了拟物化，原来的文字（比如“Edit”，“Done”之类）改为了简单的文字，原来的图标（比如新建或者删除）也做了简化。
* 程序打开加入了动画：从主界面到图标所在位置的一个放大，同时显示应用的载入界面。

自己实验了几个现有的AppStore应用在iOS7上的运行情况：

* [Pomodoro Do](https://itunes.apple.com/app/id533469911?mt=8)： 这是我自己开发的应用，运行正常，但是因为不是iOS7 SDK打包，所以在UI上使用了之前系统的，问题是导航栏Tint颜色丢失，导致很难看，需要尽快更新。
* Facebook：因为使用了图片自定义导航栏，而没有直接使用系统提供的材质，所以没什么问题。
* 面包旅行：直接Crash，无法打开，原因未知。



这次UI大改可以说是一次对敏捷开发的检验，原来的应用（特别是拟物化用得比较重的应用）虽然也能运行，但是很多UI自定义的地方需要更改不说，还容易让用户产生一种“来到了另一个世界”的感觉，同时可以看到也有部分应用无法运行。而对于苹果的封闭系统和只升不降的特性，开发者以及其应用必须要尽快适应这个新系统，这对于迭代快速，还在继续维护的应用来说会是一个机会。相信谁先能适应新的UI，谁就将在iOS7上占到先机。

#### UIKit的力学模型（UIKit Dynamics）
新增了`UIDynamicItem`委托，用来为UIView制定力学模型行为，当然其他任何对象都能通过实现这组接口来定义动力学行为，只不过在UIKit中可能应用最多。所谓动力学行为，是指将现实世界的我们常见的力学行为或者特性引入到UI中，比如重力等。通过实现UIDynamicItem，UIKit现在支持如下行为：

* UIAttachmentBehavior 连接两个实现了UIDynamicItem的物体（以下简称动力物体），一个物体移动时，另一个跟随移动
* UICollisionBehavior 指定边界，使两个动力物体可以进行碰撞
* UIGravityBehavior 顾名思义，为动力物体增加重力模拟
* UIPushBehavior 为动力物体施加持续的力
* UISnapBehavior 为动力物体指定一个附着点，想象一下类似挂一幅画在图钉上的感觉

如果有开发游戏的童鞋可能会觉得这些很多都是做游戏时候的需求，一种box2d之类的2D物理引擎的既视感跃然而出。没错的亲，动态UI，加上之后要介绍的Sprite Kit，极大的扩展了使用UIKit进行游戏开发的可能性。另外要注意UIDynamicItem不仅适用于UIKit，任何对象都可以实现接口来获得动态物体的一些特性，所以说用来做一些3D的或者其他奇怪有趣的事情也不是没有可能。如果觉得Cocos2D+box2d这样的组合使用起来不方便的话，现在动态UIKit+SpriteKit给出了新的选择。

### 游戏方面

iOS7 SDK极大加强了直接使用iOS SDK制作和分发游戏的体验，最主要的是引入了专门的游戏制作框架。

#### Sprite Kit Framework
这是个人认为iOS7 SDK最大的亮点，也是最重要的部分，iOS SDK终于有自己的精灵系统了。Sprite Kit Framework使用硬件加速的动画系统来表现2D和2.5D的游戏，它提供了制作游戏所需要的大部分的工具，包括图像渲染，动画系统，声音播放以及图像模拟的物理引擎。可以说这个框架是iOS SDK自带了一个较完备的2D游戏引擎，力图让开发者专注于更高层的实现和内容。和大多数游戏引擎一样，Sprite Kit内的内容都按照场景（Scene）来分开组织，一个场景可以包括贴图对象，视频，形状，粒子效果甚至是CoreImage滤镜等等。相对于现有的2D引擎来说，由于Sprite Kit是在系统层级进行的优化，渲染时间等都由框架决定，因此应该会有比较高的效率。

另外，Xcode还提供了创建粒子系统和贴图Atlas的工具。使用Xcode来管理粒子效果和贴图atlas，可以迅速在Sprite Kit中反应出来。

#### Game Controller Framework
为Made-for-iPhone/iPod/iPad (MFi) game controller设计的硬件的对应的框架，可以让用户用来连接和控制专门的游戏硬件。参考WWDC 2013开场视频中开始的赛车演示。现在想到的是，也许这货不仅可以用于游戏…或者苹果之后会扩展其应用，因为使用普及率很高的iPhone作为物联网的入口，似乎会是很有前途的事情。

#### GameCenter改进
GameCenter一直是苹果的败笔...虽然每年都在改进，但是一直没看到大的起色。今年也不例外，都是些小改动，不提也罢。

### 多任务强化

* 经常需要下载新内容的应用现在可以通过设置`UIBackgroundModes`为`fetch`来实现后台下载内容了，需要在AppDelegate里实现`setMinimumBackgroundFetchInterval:`以及`application:performFetchWithCompletionHandler: `来处理完成的下载，这个为后台运行代码提供了又一种选择。不过考虑到Apple如果继续严格审核的话，可能只有杂志报刊类应用能够取得这个权限吧。另外需要注意开发者仅只能指定一个最小间隔，最后下没下估计就得看系统娘的心情了。
* 同样是后台下载，以前只能推送提醒用户进入应用下载，现在可以接到推送并在后台下载。UIBackgroundModes设为remote-notification，并实现`application:didReceiveRemoteNotification:fetchCompletionHandler:`

为后台下载，开发者必须使用一个新的类`NSURLSession`，其实就是在NSURLConnection上加了个后台处理，使用类似，API十分简单，不再赘述。

### AirDrop
这个是iOS7的重头新功能，用户可以用它来分享照片，文档，链接，或者其他数据给附近的设备。但是不需要特别的实现，被集成在了标准的UIActivityViewController里，并没有单独的API提供。数据的话，可以通过实现UIActivityItemSource接口后进行发送。大概苹果也不愿意看到超出他们控制的文件分享功能吧，毕竟这和iOS设计的初衷不一样。如果你不使用UIActivityViewController的话，可能是无法在应用里实装AirDrop功能了。

### 地图
Apple在继续在地图应用上的探索，MapKit的改进也乏善可陈。我一直相信地图类应用的瓶颈一定在于数据，但是对于数据源的建立并不是一年两年能够完成的。Google在这一块凭借自己的搜索引擎有着得天独厚的优势，苹果还差的很远很远。看看有哪些新东西吧：

* MKMapCamera，可以将一个MKMapCamera对象添加到地图上，在指明位置，角度和方向后将呈现3D的样子…大概可以想象成一个数字版的Google街景..
* MKDirections 获取Apple提供的基于方向的路径，然后可以用来将路径绘制在自己的应用中。这可能对一些小的地图服务提供商产生冲击，但是还是那句话，地图是一个数据的世界，在拥有完备数据之前，Apple不是Google的对手。这个状况至少会持续好几年（也有可能是永远）。
* MKGeodesicPolyline 创建一个随地球曲率的线，并附加到地图上，完成一些视觉效果。
* MKMapSnapshotter 使用其拍摄基于地图的照片，也许各类签到类应用会用到
* 改变了overlay物件的渲染方式

### Inter-App Audio 应用间的音频
AudioUnit框架中加入了在同一台设备不同应用之间发送MIDI指令和传送音频的能力。比如在一个应用中使用AudioUnit录音，然后在另一个应用中打开以处理等。在音源应用中声明一个AURemoteIO实例来标为Inter-App可用，在目标应用中使用新的发现接口来发现并获取音频。
想法很好，也算是在应用内共享迈出了一步，不过我对现在使用AudioUnit这样的低层级框架的应用数量表示不乐观。也许今后会有一些为更高层级设计的共享API提供给开发者使用。毕竟要从AudioUnit开始处理音频对于大多数开发者来说并不是一件很容易的事情。

### 点对点连接 Peer-to-Peer Connectivity
可以看成是AirDrop不能直接使用的补偿，代价是需要自己实现。MultipeerConnectivity框架可以用来发现和连接附近的设备，并传输数据，而这一切并不需要有网络连接。可以看到Apple逐渐在文件共享方面一步步放开限制，但是当然所有这些都还是被限制在sandbox里的。

### Store Kit Framework
Store Kit在内购方面采用了新的订单系统，这将可以实现对订单的本机验证。这是一次对应内购破解和有可能验证失败导致内购失败的更新，苹果希望藉此减少内购的实现流程，减少出错，同时遏制内购破解泛滥。前者可能没有问题，但是后者的话，因为objc的动态特性，决定了只要有越狱存在，内购破解也是早晚的事情。不过这一点确实方便了没有能力架设验证服务器的小开发者，这方面来说还是很好的。

### 最后
当然还有一些其他小改动，包括MessageUI里添加了附件按钮，Xcode开始支持模块了等等。完整的iOS7新特性列表可以在[这里](https://developer.apple.com/library/prerelease/ios/releasenotes/General/WhatsNewIniOS/Articles/iOS7.html#//apple_ref/doc/uid/TP40013162-SW1)找到（暂时应该也需要开发者账号）。最后一个好消息是，苹果放慢了废弃API的速度，这个版本并没有特别重要的API被标为Deprecated，Cheers。
-->

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[iOS中使用blend改变图片颜色]]></title>
    <link href="http://onevcat.com/2013/04/using-blending-in-ios/"/>
    <updated>2013-04-29T16:30:00+09:00</updated>
    <id>http://onevcat.com/2013/04/using-blending-in-ios</id>
    <content type="html"><![CDATA[<p><img src="http://img.onevcat.com/2013/blend_title.png" alt="使用Blend处理图片颜色" /></p>

<p>最近对<code>Core Animation</code>和<code>Core Graphics</code>的内容东西比较感兴趣，自己之前也在这块相对薄弱，趁此机会也想补习一下这块的内容，所以之后几篇可能都会是对CA和CG学习的记录的文章。</p>

<p>在应用里一个很常见的需求是主题变换：同样的图标，同样的素材，但是需要按照用户喜爱变为不同的颜色。在iOS5和6的SDK里部分标准控件引入了<code>tintColor</code>，来满足个性化界面的需求，但是Apple在这方面还远远做的不够。一是现在用默认控件根本难以做出界面优秀的应用，二是<code>tintColor</code>所覆盖的并不够全面，在很多情况下开发者都无法使用其来完成个性化定义。解决办法是什么？最简单当然是拜托设计师大大出图，想要蓝色主题？那好，开PS盖个蓝色图层出一套蓝色的UI；又想加粉色UI，那好，再出一套粉色的图然后导入Xcode。代码上的话根据颜色需求使用image-blue或者image-pink这样的名字来加载图片。</p>

<p>如果有一丁点重构的意识，就会知道这不是一个很好的解决方案。工程中存在大量的冗余和重复（就算你要狡辩这些图片颜色不同不算重复，你也会在内心里知道这种狡辩是多么无力），这是非常致命的。想象一下如果你有10套主题界面，先不论应用的体积会膨胀到多少，光是想做一点修改就会痛苦万分，比如希望改一下某个按钮的形状，很好，设计师大大请重复地修改10遍，并出10套UI，然后一系列的重命名，文件移动和导入…一场灾难。</p>

<p>当然有其他办法，因为说白了就是tint不同的颜色到图片上而已，如果我们能实现改变UIImage的颜色，那我们就只需要一套UI，然后用代码来改变UI的颜色就可以了，生活有木有一下光明起来呀。嗯，让我们先从一张图片开始吧～下面是一张带有alpha通道的图片，原始颜色是纯的灰色（当然什么颜色都可以，只不过我这个人现在暂时比较喜欢灰色而已）。</p>

<!-- more -->


<p><img src="http://img.onevcat.com/2013/blend_origin.png" alt="要处理的原图" /></p>

<p>我们将用blending给这张图片加上另一个纯色作为tint，并保持原来的alpha通道。用Core Graphics来做的话，大概的想法很直接：</p>

<ol>
<li>创建一个上下文用以画新的图片</li>
<li>将新的tintColor设置为填充颜色</li>
<li>将原图片画在创建的上下文中，并用新的填充色着色（注意保持alpha通道）</li>
<li>从当前上下文中取得图片并返回</li>
</ol>


<p>最麻烦的部分可能就是保持alpha通道了。<a href="https://developer.apple.com/library/ios/#documentation/UIKit/Reference/UIImage_Class/Reference/Reference.html">UIImage的文档</a>中提供了使用blend绘图的方法<code>drawInRect:blendMode:alpha:</code>，<code>rect</code>和<code>alpha</code>都没什么问题，但是<code>blendMode</code>是个啥玩意儿啊…继续看文档，关于<a href="https://developer.apple.com/library/ios/#documentation/GraphicsImaging/Reference/CGContext/Reference/reference.html#//apple_ref/doc/c_ref/CGBlendMode"><code>CGBlendMode</code>的文档</a>，里面有一大堆看不懂的枚举值，比如这样：</p>

<p><code>
kCGBlendModeDestinationOver
R = S*(1 - Da) + D
Available in iOS 2.0 and later.
Declared in CGContext.h.
</code></p>

<p>完全不懂..直接看之后的Discussion部分：</p>

<blockquote><p>The blend mode constants introduced in OS X v10.5 represent the Porter-Duff blend modes. The symbols in the equations for these blend modes are:<br/>
R is the premultiplied result<br/>
S is the source color, and includes alpha<br/>
D is the destination color, and includes alpha<br/>
Ra, Sa, and Da are the alpha components of R, S, and D</p></blockquote>

<p>原来如此，R表示结果，S表示包含alpha的原色，D表示包含alpha的目标色，Ra，Sa和Da分别是三个的alpha。明白了这些以后，就可以开始寻找我们所需要的blend模式了。相信你可以和我一样，很快找到这个模式：</p>

<p><code>
kCGBlendModeDestinationIn
R = D*Sa
Available in iOS 2.0 and later.
Declared in CGContext.h.
</code></p>

<p>结果 = 目标色和原色透明度的加成，看起来正式所需要的。啦啦啦，还等什么呢，开始动手实现看看对不对吧～</p>

<p>为了以后使用方便，当然是祭出Category，先创建一个UIImage的类别：</p>

<p>```objc
//  UIImage+Tint.h</p>

<h1>import &lt;UIKit/UIKit.h></h1>

<p>@interface UIImage (Tint)</p>

<ul>
<li>(UIImage <em>) imageWithTintColor:(UIColor </em>)tintColor;</li>
</ul>


<p>@end
```
暂时先这样，当然我们也可以创建一个类方法直接完成从bundle读取图片然后加tintColor，但是很多时候并不如上面一个实例方法方便（比如想要从非bundle的地方获取图片），这个问题之后再说。那么就按照之前设想的步骤来实现吧：</p>

<p>```objc
//  UIImage+Tint.m</p>

<h1>import "UIImage+Tint.h"</h1>

<p>@implementation UIImage (Tint)
- (UIImage <em>) imageWithTintColor:(UIColor </em>)tintColor
{</p>

<pre><code>//We want to keep alpha, set opaque to NO; Use 0.0f for scale to use the scale factor of the device’s main screen.
UIGraphicsBeginImageContextWithOptions(self.size, NO, 0.0f);
[tintColor setFill];
CGRect bounds = CGRectMake(0, 0, self.size.width, self.size.height);
UIRectFill(bounds);

//Draw the tinted image in context
[self drawInRect:bounds blendMode:kCGBlendModeDestinationIn alpha:1.0f];

UIImage *tintedImage = UIGraphicsGetImageFromCurrentImageContext();
UIGraphicsEndImageContext();

return tintedImage;
</code></pre>

<p>}
@end
```</p>

<p>简单明了，没有任何难点。测试之：<code>[[UIImage imageNamed:@"image"] imageWithTintColor:[UIColor orangeColor]];</code>，得到的结果为：</p>

<p><img src="http://img.onevcat.com/2013/blend_1.png" alt="使用kCGBlendModeDestinationIn模式的结果" /></p>

<p>嗯...怎么说呢，虽然tintColor的颜色是变了，但是总觉得怪怪的。仔细对比一下就会发现，原来灰色图里星星和周围的灰度渐变到了橙色的图里好像都消失了：星星整个变成了橙色，周围的一圈漂亮的光晕也没有了，这是神马情况啊…这种图能交差的话那算见鬼了，会被设计和产品打死的吧。对于无渐变的纯色图的图来说直接用上面的方法是没问题的，但是现在除了Metro的大色块以外哪里无灰度渐变的设计啊…检查一下使用的blend，<code>R = D * Sa</code>，恍然大悟，我们虽然保留了原色的透明度，但是却把它的所有的灰度信息弄丢了。怎么办？继续刨<code>CGBlendMode</code>的文档吧，那么多blend模式应该总有我们需要的。功夫不负有心人，<code>kCGBlendModeOverlay</code>一副嗷嗷待选的样子：</p>

<p><code>
kCGBlendModeOverlay
Either multiplies or screens the source image samples with the background image samples, depending on the background color. The result is to overlay the existing image samples while preserving the highlights and shadows of the background. The background color mixes with the source image to reflect the lightness or darkness of the background.
Available in iOS 2.0 and later.
Declared in CGContext.h.
</code></p>

<p>kCGBlendModeOverlay可以保持背景色的明暗，也就是灰度信息，听起来正是我们需要的。加入到声明中，并且添加相应的实现( 顺便重构一下原来的代码 :) )：</p>

<p>```objc
//  UIImage+Tint.h</p>

<h1>import &lt;UIKit/UIKit.h></h1>

<p>@interface UIImage (Tint)</p>

<ul>
<li>(UIImage <em>) imageWithTintColor:(UIColor </em>)tintColor;</li>
<li>(UIImage <em>) imageWithGradientTintColor:(UIColor </em>)tintColor;</li>
</ul>


<p>@end
```</p>

<p>```objc
//  UIImage+Tint.m</p>

<h1>import "UIImage+Tint.h"</h1>

<p>@implementation UIImage (Tint)
- (UIImage <em>) imageWithTintColor:(UIColor </em>)tintColor
{</p>

<pre><code>return [self imageWithTintColor:tintColor blendMode:kCGBlendModeDestinationIn];
</code></pre>

<p>}</p>

<ul>
<li><p>(UIImage <em>) imageWithGradientTintColor:(UIColor </em>)tintColor
{
  return [self imageWithTintColor:tintColor blendMode:kCGBlendModeOverlay];
}</p></li>
<li><p>(UIImage <em>) imageWithTintColor:(UIColor </em>)tintColor blendMode:(CGBlendMode)blendMode
{
  //We want to keep alpha, set opaque to NO; Use 0.0f for scale to use the scale factor of the device’s main screen.
  UIGraphicsBeginImageContextWithOptions(self.size, NO, 0.0f);
  [tintColor setFill];
  CGRect bounds = CGRectMake(0, 0, self.size.width, self.size.height);
  UIRectFill(bounds);</p>

<p>  //Draw the tinted image in context
  [self drawInRect:bounds blendMode:blendMode alpha:1.0f];</p>

<p>  UIImage *tintedImage = UIGraphicsGetImageFromCurrentImageContext();
  UIGraphicsEndImageContext();</p>

<p>  return tintedImage;
}</p></li>
</ul>


<p>@end
```</p>

<p>完成，测试之…好吧，好尴尬，虽然颜色和周围的光这次对了，但是透明度又没了啊魂淡..一点不奇怪啊，因为<code>kCGBlendModeOverlay</code>本来就没承诺给你保留原图的透明度的说。</p>

<p><img src="http://img.onevcat.com/2013/blend_2.png" alt="使用kCGBlendModeOverlay模式的结果" /></p>

<p>那么..既然我们用<code>kCGBlendModeOverlay</code>能保留灰度信息，用<code>kCGBlendModeDestinationIn</code>能保留透明度信息，那就两个blendMode都用不就完事儿了么～尝试之，如果在blend绘图时不是<code>kCGBlendModeDestinationIn</code>模式的话，则再用<code>kCGBlendModeDestinationIn</code>画一次：</p>

<p>```objc
//  UIImage+Tint.m</p>

<h1>import "UIImage+Tint.h"</h1>

<p>@implementation UIImage (Tint)
- (UIImage <em>) imageWithTintColor:(UIColor </em>)tintColor
{</p>

<pre><code>return [self imageWithTintColor:tintColor blendMode:kCGBlendModeDestinationIn];
</code></pre>

<p>}</p>

<ul>
<li><p>(UIImage <em>) imageWithGradientTintColor:(UIColor </em>)tintColor
{
  return [self imageWithTintColor:tintColor blendMode:kCGBlendModeOverlay];
}</p></li>
<li><p>(UIImage <em>) imageWithTintColor:(UIColor </em>)tintColor blendMode:(CGBlendMode)blendMode
{
  //We want to keep alpha, set opaque to NO; Use 0.0f for scale to use the scale factor of the device’s main screen.
  UIGraphicsBeginImageContextWithOptions(self.size, NO, 0.0f);
  [tintColor setFill];
  CGRect bounds = CGRectMake(0, 0, self.size.width, self.size.height);
  UIRectFill(bounds);</p>

<p>  //Draw the tinted image in context
  [self drawInRect:bounds blendMode:blendMode alpha:1.0f];</p>

<p>  if (blendMode != kCGBlendModeDestinationIn) {</p>

<pre><code>  [self drawInRect:bounds blendMode:kCGBlendModeDestinationIn alpha:1.0f];
</code></pre>

<p>  }</p>

<p>  UIImage *tintedImage = UIGraphicsGetImageFromCurrentImageContext();
  UIGraphicsEndImageContext();</p>

<p>  return tintedImage;
}</p></li>
</ul>


<p>@end
```</p>

<p>结果如下：</p>

<p><img src="http://img.onevcat.com/2013/blend_3.png" alt="使用kCGBlendModeOverlay和kCGBlendModeDestinationIn模式的结果" /></p>

<p>已经很完美了，这样的话只要在代码里设定一下颜色，我们就能够很轻易地使用同样一套UI，将其blend为需要的颜色，来实现素材的重用了。唯一需要注意的是，因为每次使用<code>UIImage+Tint</code>的方法绘图时，都使用了CG的绘制方法，这就意味着每次调用都会是用到CPU的Offscreen drawing，大量使用的话可能导致性能的问题（主要对于iPhone 3GS或之前的设备，可能同时处理大量这样的绘制调用的能力会有不足）。关于CA和CG的性能的问题，打算在之后用一篇文章来介绍一下。对于这里的<code>UIImage+Tint</code>的实现，可以写一套缓存的机制，来确保大量重复的元素只在load的时候blend一次，之后将其缓存在内存中以快速读取。当然这是一个权衡的问题，在时间和空间中做出正确的平衡和选择，也正是程序设计的乐趣所在。</p>

<p>这篇文章中作为示例的工程和UIImage+Tint可以在<a href="https://github.com/onevcat/VVImageTint">Github</a>上找到，您可以随意玩弄..我相信也会是个来研究每种blend的特性的好机会～</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[两个人一起记账吧～ Our Money]]></title>
    <link href="http://onevcat.com/2013/04/our-money-app/"/>
    <updated>2013-04-06T11:54:00+09:00</updated>
    <id>http://onevcat.com/2013/04/our-money-app</id>
    <content type="html"><![CDATA[<p><a href="https://itunes.apple.com/cn/app/our-money/id562520527?ls=1&amp;mt=8"><img src="http://img.onevcat.com/2013/ourmoney-banner.jpg" alt="image" /></a></p>

<p>Our Money是一款能够协助多人在云端记账的iOS应用，可以帮助您简单地记录和整理日常开销，您可以邀请您的朋友和家人与您一起记账，免去每日汇报总结之苦。</p>

<ul>
<li><a href="https://itunes.apple.com/cn/app/our-money/id562520527?ls=1&amp;mt=8">App Store地址</a></li>
<li><a href="http://ourmoney.onevcat.com">Our Money app的首页</a></li>
</ul>


<p>大概但凡从按月领生活费开始花钱以来，都会兴起记账的念头，至于是否能够坚持，就各凭本事了。说到自己，则是多次付诸行动，然后不了了之。从一开始记在小本本上自己用计算器加加减减，到建个Excel文档自动求和，再到手机上的记账应用，时代在进步，咱的手段也在进步，却总还觉得没有找到最合适的工具。尤其是用手机记账以来，有的软件，每次对非得给一笔开销定义出两层的分类，让我头疼不已，家庭小帐非得整成个公司帐簿，改动标签也颇为麻烦；有的软件，记录条目倒是简单，但其他诸如统计等功能却也一起被简化了。不过，最让我郁闷的是，记账总成为我一个人的事情，谁让是用我的手机在记呢。</p>

<p>现在，终于等到了一款操作简单但是功能齐全，尤其是，<strong>可以多人共同记账的应用</strong>。这款叫做Our Money 的应用，最大的亮点当然就在于“Our”。它可以实现多人一起记账，只要人手一个应用，就可以和家人一起记录家庭开销，和朋友一起整理出游费用，不同的帐本可以选择和不同的人分享，每个人都能参与，条目更新实时同步，再不用一个人负责所有的帐目。</p>

<p>好啦，废话不多说，让我们一起来体验一下这个软件吧。<a href="https://itunes.apple.com/cn/app/our-money/id562520527?ls=1&amp;mt=8">下载应用</a>并打开，用邮箱注册用户，就可以开始记账啦。请记住你的邮箱是你邀请别人或者别人邀请你共同记账的标识哦～</p>

<!-- more -->




<br>


<p><img src="http://img.onevcat.com/2013/1-ourmoney-login.png" alt="OurMoney的注册登陆界面" />
Our Money的主界面相当简洁，最上方列出列表名称，收入（预算）、支出、结余也一目了然，条目的时间、分类、备注都一目了然。那么其他其他内容被藏在哪里呢？左边一拉，当前列表的按月总计；右边一拉，列表编辑，数据统计，就是这么简单～</p>

<br>


<p><img src="http://img.onevcat.com/2013/2-ourmoney-month.png" alt="按月份统计收入和开销" /></p>

<br>


<p><img src="http://img.onevcat.com/2013/3-ourmoney-stat.png" alt="按项目和用户的统计" />
首先我们新建一个列表， 在右边的界面下拉一下，就可以新建自己的列表了。选中的列表下方能够修改列表名称或者删除，中间的邀请就是重头戏啦，输入希望一同记账的朋友的邮箱，他就可以收到邀请并加入你的列表。当邀请了朋友或家人加入列表后，列表信息中就会显示多人同为列表用户。当然，在记账时随时可以邀请新的用户加入。</p>

<br>


<p><img src="http://img.onevcat.com/2013/4-ourmoney-invite.png" alt="邀请别人加入特定列表一起记账" />
选定刚才新建的列表，回到主界面，随便记下一点东西，在同一列表中的用户将通过推送（如果允许的话）收到您更改了列表的消息。而对方打开应用时，马上就可以同步地看到您所记录的信息，这便于双方更迅速地各自完成记账，免去了回家后苦苦思索或者汇总的麻烦，确实十分方便。</p>

<br>


<p><img src="http://img.onevcat.com/2013/5-ourmoney-push.png" alt="家人或朋友记账后，立即可以收到系统提醒" /></p>

<p>记错了，找不到修改的地方怎么办？点一下，记录被选中，下面就出现了编辑或者删除的选项，还可以分享条目到社交网络，秀一下收到的礼物什么的哦～</p>

<p>在消费和记账时难免会出现没有网络的尴尬时候，这时候Our Money还能正常工作么？当然，Our Money具有完善的离线模式处理，没有网络时照常使用，当之后连上网络的时候会自动为您完成所有同步，完全不用自己操心。</p>

<br>


<p><img src="http://img.onevcat.com/2013/6-ourmoney-offline.png" alt="Our Money方便的离线模式" /></p>

<p>总的来说Our Money是一款功能强大但又简单高效的记账软件，其云端记账和共同记账的理念很符合当今多人记账的需求。从今天开始就和家人朋友用Our Money一起记账吧～</p>

<p>您可以从<a href="https://itunes.apple.com/cn/app/our-money/id562520527?ls=1&amp;mt=8">App Store中下载Our Money</a>，还可以进一步通过应用内的赠送系统将您的记账和心得分享给家人朋友。</p>
]]></content>
  </entry>
  
</feed>
